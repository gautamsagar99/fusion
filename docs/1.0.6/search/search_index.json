{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PyFusion \u00b6 PyFusion is the Python SDK for the Fusion platform API. Installation \u00b6 pip install pyfusion Fusion by J.P. Morgan is a cloud-native data platform for institutional investors, providing end-to-end data management, analytics, and reporting solutions across the investment lifecycle. The platform allows clients to seamlessly integrate and combine data from multiple sources into a single data model that delivers the benefits and scale and reduces costs, along with the ability to more easily unlock timely analysis and insights. Fusion's open data architecture supports flexible distribution, including partnerships with cloud and data providers, all managed by J.P. Morgan data experts. For more information, please visit fusion.jpmorgan.com","title":"Home"},{"location":"#pyfusion","text":"PyFusion is the Python SDK for the Fusion platform API.","title":"PyFusion"},{"location":"#installation","text":"pip install pyfusion Fusion by J.P. Morgan is a cloud-native data platform for institutional investors, providing end-to-end data management, analytics, and reporting solutions across the investment lifecycle. The platform allows clients to seamlessly integrate and combine data from multiple sources into a single data model that delivers the benefits and scale and reduces costs, along with the ability to more easily unlock timely analysis and insights. Fusion's open data architecture supports flexible distribution, including partnerships with cloud and data providers, all managed by J.P. Morgan data experts. For more information, please visit fusion.jpmorgan.com","title":"Installation"},{"location":"api/","text":"Top-level package for fusion. authentication \u00b6 Fusion authentication module. FusionCredentials \u00b6 Utility functions to manage credentials. __init__ ( self , client_id = None , client_secret = None , username = None , password = None , resource = None , auth_url = None , proxies = {}, grant_type = 'client_credentials' ) special \u00b6 Constructor for the FusionCredentials authentication management class. Parameters: Name Type Description Default client_id str A valid OAuth client identifier. Defaults to None. None client_secret str A valid OAuth client secret. Defaults to None. None username str A valid username. Defaults to None. None password str A valid password for the username. Defaults to None. None resource str The OAuth audience. Defaults to None. None auth_url str URL for the OAuth authentication server. Defaults to None. None proxies dict Any proxy servers required to route HTTP and HTTPS requests to the internet. {} grant_type str Allows the grant type to be changed to support different credential types. Defaults to client_credentials. 'client_credentials' Source code in fusion/authentication.py def __init__ ( self , client_id : str = None , client_secret : str = None , username : str = None , password : str = None , resource : str = None , auth_url : str = None , proxies = {}, grant_type : str = 'client_credentials' , ) -> None : \"\"\"Constructor for the FusionCredentials authentication management class. Args: client_id (str, optional): A valid OAuth client identifier. Defaults to None. client_secret (str, optional): A valid OAuth client secret. Defaults to None. username (str, optional): A valid username. Defaults to None. password (str, optional): A valid password for the username. Defaults to None. resource (str, optional): The OAuth audience. Defaults to None. auth_url (str, optional): URL for the OAuth authentication server. Defaults to None. proxies (dict, optional): Any proxy servers required to route HTTP and HTTPS requests to the internet. grant_type (str, optional): Allows the grant type to be changed to support different credential types. Defaults to client_credentials. \"\"\" self . client_id = client_id self . client_secret = client_secret self . username = username self . password = password self . resource = resource self . auth_url = auth_url self . proxies = proxies self . grant_type = grant_type add_proxies ( http_proxy , https_proxy = None , credentials_file = 'config/client_credentials.json' ) staticmethod \u00b6 A function to add proxies to an existing credentials files. This function can be called to add proxy addresses to a credential file downloaded from the Fusion website. Parameters: Name Type Description Default http_proxy str The HTTP proxy address. required https_proxy str The HTTPS proxy address. If not specified then this will be the copied form the HTTP proxy. None credentials_file str The path and filename to store the credentials under. Path may be absolute or relative to current working directory. Defaults to 'config/client_credentials.json'. 'config/client_credentials.json' Returns: Type Description None None Source code in fusion/authentication.py @staticmethod def add_proxies ( http_proxy : str , https_proxy : str = None , credentials_file : str = 'config/client_credentials.json' ) -> None : \"\"\"A function to add proxies to an existing credentials files. This function can be called to add proxy addresses to a credential file downloaded from the Fusion website. Args: http_proxy (str): The HTTP proxy address. https_proxy (str): The HTTPS proxy address. If not specified then this will be the copied form the HTTP proxy. credentials_file (str, optional): The path and filename to store the credentials under. Path may be absolute or relative to current working directory. Defaults to 'config/client_credentials.json'. Returns: None \"\"\" credentials = FusionCredentials . from_file ( credentials_file ) credentials . proxies [ 'http' ] = http_proxy if https_proxy is None : https_proxy = http_proxy credentials . proxies [ 'https' ] = https_proxy data : Dict [ str , Union [ str , dict ]] = dict ( { 'client_id' : credentials . client_id , 'client_secret' : credentials . client_secret , 'resource' : credentials . resource , 'auth_url' : credentials . auth_url , 'proxies' : credentials . proxies , } ) json_data = json . dumps ( data , indent = 4 ) with open ( credentials_file , 'w' ) as credentialsfile : credentialsfile . write ( json_data ) return from_dict ( credentials ) staticmethod \u00b6 Create a credentials object from a dictionary. This is the only FusionCredentials creation method that supports the password grant type since the username and password should be provided by the user. Parameters: Name Type Description Default credentials dict A dictionary containing the requried keys: client_id, client_secret, resource, auth_url, and optionally proxies and an OAuth grant type. required Returns: Type Description FusionCredentials a credentials object that can be used for authentication. Source code in fusion/authentication.py @staticmethod def from_dict ( credentials : dict ): \"\"\"Create a credentials object from a dictionary. This is the only FusionCredentials creation method that supports the password grant type since the username and password should be provided by the user. Args: credentials (dict): A dictionary containing the requried keys: client_id, client_secret, resource, auth_url, and optionally proxies and an OAuth grant type. Returns: FusionCredentials: a credentials object that can be used for authentication. \"\"\" if 'grant_type' in credentials : grant_type = credentials [ 'grant_type' ] else : grant_type = 'client_credentials' client_id = credentials [ 'client_id' ] if grant_type == 'client_credentials' : client_secret = credentials [ 'client_secret' ] username = None password = None elif grant_type == 'password' : client_secret = None username = credentials [ 'username' ] password = credentials [ 'password' ] else : raise CredentialError ( f 'Unrecognised grant type { grant_type } ' ) resource = credentials [ 'resource' ] auth_url = credentials [ 'auth_url' ] proxies = credentials . get ( 'proxies' ) creds = FusionCredentials ( client_id , client_secret , username , password , resource , auth_url , proxies , grant_type = grant_type ) return creds from_file ( file_path = 'config/client.credentials.json' , fs = None , walk_up_dirs = True ) staticmethod \u00b6 Create a credentials object from a file. Parameters: Name Type Description Default file_path str Path (absolute or relative) and filename to load credentials from. Defaults to 'config/client.credentials.json'. 'config/client.credentials.json' fs fsspec.filesystem Filesystem to use. None walk_up_dirs bool if true it walks up the directories in search of a config folder True Returns: Type Description FusionCredentials a credentials object that can be used for authentication. Source code in fusion/authentication.py @staticmethod def from_file ( file_path : str = 'config/client.credentials.json' , fs = None , walk_up_dirs = True ): \"\"\"Create a credentials object from a file. Args: file_path (str, optional): Path (absolute or relative) and filename to load credentials from. Defaults to 'config/client.credentials.json'. fs (fsspec.filesystem): Filesystem to use. walk_up_dirs (bool): if true it walks up the directories in search of a config folder Returns: FusionCredentials: a credentials object that can be used for authentication. \"\"\" fs = fs if fs else get_default_fs () to_use_file_path = None if fs . exists ( file_path ): # absolute path case logger . log ( VERBOSE_LVL , f \"Found credentials file at { file_path } \" ) to_use_file_path = file_path elif fs . exists ( os . path . join ( fs . info ( \"\" )[ \"name\" ], file_path )): # relative path case to_use_file_path = os . path . join ( fs . info ( \"\" )[ \"name\" ], file_path ) logger . log ( VERBOSE_LVL , f \"Found credentials file at { to_use_file_path } \" ) else : for p in [ s . __str__ () for s in Path ( fs . info ( \"\" )[ \"name\" ]) . parents ]: if fs . exists ( os . path . join ( p , file_path )): to_use_file_path = os . path . join ( p , file_path ) logger . log ( VERBOSE_LVL , f \"Found credentials file at { to_use_file_path } \" ) break if fs . size ( to_use_file_path ) > 0 : try : with fs . open ( to_use_file_path , 'r' ) as credentials : data = json . load ( credentials ) credentials = FusionCredentials . from_dict ( data ) return credentials except Exception as e : print ( e ) logger . error ( e ) raise Exception ( e ) else : msg = f \" { to_use_file_path } is an empty file, make sure to add your credentials to it.\" print ( msg ) logger . error ( msg ) raise IOError ( msg ) from_object ( credentials_source ) staticmethod \u00b6 Utility function that will determine how to create a credentials object based on data passed. Parameters: Name Type Description Default credentials_source Union[str, dict] A string which could be a filename or a JSON object, or a dictionary. required Exceptions: Type Description CredentialError Exception raised when the provided credentials is not one of the supported types Returns: Type Description FusionCredentials a credentials object that can be used for authentication. Source code in fusion/authentication.py @staticmethod def from_object ( credentials_source : Union [ str , dict ]): \"\"\"Utility function that will determine how to create a credentials object based on data passed. Args: credentials_source (Union[str, dict]): A string which could be a filename or a JSON object, or a dictionary. Raises: CredentialError: Exception raised when the provided credentials is not one of the supported types Returns: FusionCredentials: a credentials object that can be used for authentication. \"\"\" if isinstance ( credentials_source , dict ): return FusionCredentials . from_dict ( credentials_source ) elif isinstance ( credentials_source , str ): if _is_json ( credentials_source ): return FusionCredentials . from_dict ( json . loads ( credentials_source )) else : return FusionCredentials . from_file ( credentials_source ) raise CredentialError ( f 'Could not resolve the credentials provided: { credentials_source } ' ) generate_credentials_file ( credentials_file = 'config/client_credentials.json' , client_id = None , client_secret = None , resource = 'JPMC:URI:RS-93742-Fusion-PROD' , auth_url = 'https://authe.jpmorgan.com/as/token.oauth2' , proxies = None ) staticmethod \u00b6 Utility function to generate credentials file that can be used for authentication. Parameters: Name Type Description Default credentials_file str The path and filename to store the credentials under. Path may be absolute or relative to current working directory. Defaults to 'config/client_credentials.json'. 'config/client_credentials.json' client_id str A valid OAuth client identifier. Defaults to None. None client_secret str A valid OAuth client secret. Defaults to None. None resource str The OAuth audience. Defaults to None. 'JPMC:URI:RS-93742-Fusion-PROD' auth_url str URL for the OAuth authentication server. Defaults to None. 'https://authe.jpmorgan.com/as/token.oauth2' proxies Union[str, dict] Any proxy servers required to route HTTP and HTTPS requests to the internet. Defaults to {}. Keys are http and https. Or specify a single URL to set both http and https None Exceptions: Type Description CredentialError Exception to handle missing values required for authentication. Returns: Type Description FusionCredentials a credentials object that can be used for authentication. Source code in fusion/authentication.py @staticmethod def generate_credentials_file ( credentials_file : str = 'config/client_credentials.json' , client_id : str = None , client_secret : str = None , resource : str = \"JPMC:URI:RS-93742-Fusion-PROD\" , auth_url : str = \"https://authe.jpmorgan.com/as/token.oauth2\" , proxies : Union [ str , dict ] = None , ): \"\"\"Utility function to generate credentials file that can be used for authentication. Args: credentials_file (str, optional): The path and filename to store the credentials under. Path may be absolute or relative to current working directory. Defaults to 'config/client_credentials.json'. client_id (str, optional): A valid OAuth client identifier. Defaults to None. client_secret (str, optional): A valid OAuth client secret. Defaults to None. resource (str, optional): The OAuth audience. Defaults to None. auth_url (str, optional): URL for the OAuth authentication server. Defaults to None. proxies (Union[str, dict], optional): Any proxy servers required to route HTTP and HTTPS requests to the internet. Defaults to {}. Keys are http and https. Or specify a single URL to set both http and https Raises: CredentialError: Exception to handle missing values required for authentication. Returns: FusionCredentials: a credentials object that can be used for authentication. \"\"\" if not client_id : raise CredentialError ( 'A valid client_id is required' ) if not client_secret : raise CredentialError ( 'A valid client secret is required' ) data : Dict [ str , Union [ str , dict ]] = dict ( { 'client_id' : client_id , 'client_secret' : client_secret , 'resource' : resource , 'auth_url' : auth_url } ) proxies_resolved = {} if proxies : if isinstance ( proxies , dict ): raw_proxies_dict = proxies elif isinstance ( proxies , str ): if _is_url ( proxies ): raw_proxies_dict = { 'http' : proxies , 'https' : proxies } elif _is_json ( proxies ): raw_proxies_dict = json . loads ( proxies ) else : raise CredentialError ( f 'A valid proxies param is required, [ { proxies } ] is not supported.' ) # Now validate and conform proxies dict valid_pxy_keys = [ 'http' , 'https' , 'http_proxy' , 'https_proxy' ] pxy_key_map = { 'http' : 'http' , 'https' : 'https' , 'http_proxy' : 'http' , 'https_proxy' : 'https' , } lcase_dict = { k . lower (): v for k , v in raw_proxies_dict . items ()} if set ( lcase_dict . keys ()) . intersection ( set ( valid_pxy_keys )) != set ( lcase_dict . keys ()): raise CredentialError ( f 'Invalid proxies keys in dict { raw_proxies_dict . keys () } .' f 'Only { pxy_key_map . keys () } are accepted and will be mapped as necessary.' ) proxies_resolved = { pxy_key_map [ k ]: v for k , v in lcase_dict . items ()} data [ 'proxies' ] = proxies_resolved json_data = json . dumps ( data , indent = 4 ) Path ( credentials_file ) . parent . mkdir ( parents = True , exist_ok = True ) with open ( credentials_file , 'w' ) as credentialsfile : credentialsfile . write ( json_data ) credentials = FusionCredentials . from_file ( file_path = credentials_file ) return credentials FusionOAuthAdapter \u00b6 An OAuth adapter to manage authentication and session tokens. __init__ ( self , credentials , proxies = {}, refresh_within_seconds = 5 , auth_retries = None , * args , ** kwargs ) special \u00b6 Class constructor to create a FusionOAuthAdapter object. Parameters: Name Type Description Default credentials Union[fusion.authentication.FusionCredentials, str, dict] Valid user credentials to authenticate. required proxies dict Specify a proxy if required to access the authentication server. Defaults to {}. {} refresh_within_seconds int When an API call is made with less than the specified number of seconds until the access token expires, or after expiry, it will refresh the token. Defaults to 5. 5 auth_retries Union[int, urllib3.util.retry.Retry] Number of times to attempt to authenticate to handle connection problems. Defaults to None. None Source code in fusion/authentication.py def __init__ ( self , credentials : Union [ FusionCredentials , Union [ str , dict ]], proxies : dict = {}, refresh_within_seconds : int = 5 , auth_retries : Union [ int , Retry ] = None , * args , ** kwargs , ) -> None : \"\"\"Class constructor to create a FusionOAuthAdapter object. Args: credentials (Union[FusionCredentials, Union[str, dict]): Valid user credentials to authenticate. proxies (dict, optional): Specify a proxy if required to access the authentication server. Defaults to {}. refresh_within_seconds (int, optional): When an API call is made with less than the specified number of seconds until the access token expires, or after expiry, it will refresh the token. Defaults to 5. auth_retries (Union[int, Retry]): Number of times to attempt to authenticate to handle connection problems. Defaults to None. \"\"\" super ( FusionOAuthAdapter , self ) . __init__ ( * args , ** kwargs ) if isinstance ( credentials , FusionCredentials ): self . credentials = credentials else : self . credentials = FusionCredentials . from_object ( credentials ) if proxies : self . proxies = proxies else : self . proxies = self . credentials . proxies self . bearer_token_expiry = datetime . datetime . now () self . number_token_refreshes = 0 self . refresh_within_seconds = refresh_within_seconds if not auth_retries : self . auth_retries = Retry ( total = 20 , backoff_factor = 0.2 ) else : self . auth_retries = Retry . from_int ( auth_retries ) send ( self , request , ** kwargs ) \u00b6 Function to send a request to the authentication server. Parameters: Name Type Description Default request requests.Session A HTTP Session. required Source code in fusion/authentication.py def send ( self , request , ** kwargs ): \"\"\"Function to send a request to the authentication server. Args: request (requests.Session): A HTTP Session. Returns: \"\"\" def _refresh_token_data (): payload = ( { \"grant_type\" : \"client_credentials\" , \"client_id\" : self . credentials . client_id , \"client_secret\" : self . credentials . client_secret , \"aud\" : self . credentials . resource , } if self . credentials . grant_type == 'client_credentials' else { \"grant_type\" : \"password\" , \"client_id\" : self . credentials . client_id , \"username\" : self . credentials . username , \"password\" : self . credentials . password , \"resource\" : self . credentials . resource , } ) try : s = requests . Session () if self . proxies : # mypy does note recognise session.proxies as a dict so fails this line, we'll ignore this chk s . proxies . update ( self . proxies ) # type:ignore s . mount ( 'http://' , HTTPAdapter ( max_retries = self . auth_retries )) response = s . post ( self . credentials . auth_url , data = payload ) response_data = response . json () access_token = response_data [ \"access_token\" ] expiry = response_data [ \"expires_in\" ] return access_token , expiry except Exception as ex : raise Exception ( f 'Failed to authenticate against OAuth server { ex } ' ) token_expires_in = ( self . bearer_token_expiry - datetime . datetime . now ()) . total_seconds () if token_expires_in < self . refresh_within_seconds : token , expiry = _refresh_token_data () self . token = token self . bearer_token_expiry = datetime . datetime . now () + timedelta ( seconds = int ( expiry )) self . number_token_refreshes += 1 logger . log ( VERBOSE_LVL , f 'Refreshed token { self . number_token_refreshes } time { _res_plural ( self . number_token_refreshes ) } ' , ) request . headers . update ({ 'Authorization' : f 'Bearer { self . token } ' }) response = super ( FusionOAuthAdapter , self ) . send ( request , ** kwargs ) return response get_default_fs () \u00b6 Retrieve default filesystem. Returns: filesystem Source code in fusion/authentication.py def get_default_fs (): \"\"\"Retrieve default filesystem. Returns: filesystem \"\"\" protocol = \"file\" if \"FS_PROTOCOL\" not in os . environ . keys () else os . environ [ \"FS_PROTOCOL\" ] if ( \"S3_ENDPOINT\" in os . environ . keys () and \"AWS_ACCESS_KEY_ID\" in os . environ . keys () and \"AWS_SECRET_ACCESS_KEY\" in os . environ . keys () ): endpoint = os . environ [ \"S3_ENDPOINT\" ] fs = fsspec . filesystem ( \"s3\" , client_kwargs = { \"endpoint_url\" : f \"https:// { endpoint } \" }, key = os . environ [ \"AWS_ACCESS_KEY_ID\" ], secret = os . environ [ \"AWS_SECRET_ACCESS_KEY\" ], ) else : fs = fsspec . filesystem ( protocol ) return fs exceptions \u00b6 Bespoke exceptions and errors. APIConnectError \u00b6 APIConnectError exception wrapper to handle API connection errors. Parameters: Name Type Description Default Exception Exception to wrap. required APIRequestError \u00b6 APIRequestError exception wrapper to handle API request erorrs. Parameters: Name Type Description Default Exception Exception to wrap. required APIResponseError \u00b6 APIResponseError exception wrapper to handle API response errors. Parameters: Name Type Description Default Exception Exception to wrap. required CredentialError \u00b6 CredentialError exception wrapper to handle errors in credentials provided for authentication. Parameters: Name Type Description Default Exception Exception to wrap. required UnrecognizedFormatError \u00b6 UnrecognizedFormatError exception wrapper to handle format errors. Parameters: Name Type Description Default Exception Exception to wrap. required fusion \u00b6 Main Fusion module. Fusion \u00b6 Core Fusion class for API access. default_catalog : str property writable \u00b6 Returns the default catalog. Returns: Type Description str None __init__ ( self , credentials = 'config/client_credentials.json' , root_url = 'https://fusion-api.jpmorgan.com/fusion/v1/' , download_folder = 'downloads' , log_level = 40 , fs = None ) special \u00b6 Constructor to instantiate a new Fusion object. Parameters: Name Type Description Default credentials Union[str, dict] A path to a credentials file or a dictionary containing the required keys. Defaults to 'config/client_credentials.json'. 'config/client_credentials.json' root_url str The API root URL. Defaults to \"https://fusion-api.jpmorgan.com/fusion/v1/\". 'https://fusion-api.jpmorgan.com/fusion/v1/' download_folder str The folder path where downloaded data files are saved. Defaults to \"downloads\". 'downloads' log_level int Set the logging level. Defaults to logging.ERROR. 40 fs fsspec.filesystem filesystem. None Source code in fusion/fusion.py def __init__ ( self , credentials : Union [ str , dict ] = 'config/client_credentials.json' , root_url : str = \"https://fusion-api.jpmorgan.com/fusion/v1/\" , download_folder : str = \"downloads\" , log_level : int = logging . ERROR , fs = None , ) -> None : \"\"\"Constructor to instantiate a new Fusion object. Args: credentials (Union[str, dict], optional): A path to a credentials file or a dictionary containing the required keys. Defaults to 'config/client_credentials.json'. root_url (_type_, optional): The API root URL. Defaults to \"https://fusion-api.jpmorgan.com/fusion/v1/\". download_folder (str, optional): The folder path where downloaded data files are saved. Defaults to \"downloads\". log_level (int, optional): Set the logging level. Defaults to logging.ERROR. fs (fsspec.filesystem): filesystem. \"\"\" self . _default_catalog = \"common\" self . root_url = root_url self . download_folder = download_folder Path ( download_folder ) . mkdir ( parents = True , exist_ok = True ) if logger . hasHandlers (): logger . handlers . clear () file_handler = logging . FileHandler ( filename = \"fusion_sdk.log\" ) logging . addLevelName ( VERBOSE_LVL , \"VERBOSE\" ) stdout_handler = logging . StreamHandler ( sys . stdout ) formatter = logging . Formatter ( \" %(asctime)s . %(msecs)03d %(name)s : %(levelname)s %(message)s \" , datefmt = \"%Y-%m- %d %H:%M:%S\" ) stdout_handler . setFormatter ( formatter ) logger . addHandler ( stdout_handler ) logger . addHandler ( file_handler ) logger . setLevel ( log_level ) if isinstance ( credentials , FusionCredentials ): self . credentials = credentials else : self . credentials = FusionCredentials . from_object ( credentials ) self . session = get_session ( self . credentials , self . root_url ) self . fs = fs if fs else get_default_fs () __repr__ ( self ) special \u00b6 Object representation to list all available methods. Source code in fusion/fusion.py def __repr__ ( self ): \"\"\"Object representation to list all available methods.\"\"\" return \"Fusion object \\n Available methods: \\n \" + tabulate ( pd . DataFrame ( [ [ method_name for method_name in dir ( Fusion ) if callable ( getattr ( Fusion , method_name )) and not method_name . startswith ( \"_\" ) ] + [ p for p in dir ( Fusion ) if isinstance ( getattr ( Fusion , p ), property )] ] ) . T . set_index ( 0 ), tablefmt = \"psql\" , ) catalog_resources ( self , catalog = None , output = False ) \u00b6 List the resources contained within the catalog, for example products and datasets. Parameters: Name Type Description Default catalog str A catalog identifier. Defaults to 'common'. None output bool If True then print the dataframe. Defaults to False. False Returns: Type Description DataFrame pandas.DataFrame: A dataframe with a row for each resource within the catalog Source code in fusion/fusion.py def catalog_resources ( self , catalog : str = None , output : bool = False ) -> pd . DataFrame : \"\"\"List the resources contained within the catalog, for example products and datasets. Args: catalog (str, optional): A catalog identifier. Defaults to 'common'. output (bool, optional): If True then print the dataframe. Defaults to False. Returns: pandas.DataFrame: A dataframe with a row for each resource within the catalog \"\"\" catalog = self . __use_catalog ( catalog ) url = f ' { self . root_url } catalogs/ { catalog } ' df = Fusion . _call_for_dataframe ( url , self . session ) if output : print ( tabulate ( df , headers = \"keys\" , tablefmt = \"psql\" , maxcolwidths = 30 )) return df dataset_resources ( self , dataset , catalog = None , output = False ) \u00b6 List the resources available for a dataset, currently this will always be a datasetseries. Parameters: Name Type Description Default dataset str A dataset identifier required catalog str A catalog identifier. Defaults to 'common'. None output bool If True then print the dataframe. Defaults to False. False Returns: Type Description DataFrame pandas.DataFrame: A dataframe with a row for each resource Source code in fusion/fusion.py def dataset_resources ( self , dataset : str , catalog : str = None , output : bool = False ) -> pd . DataFrame : \"\"\"List the resources available for a dataset, currently this will always be a datasetseries. Args: dataset (str): A dataset identifier catalog (str, optional): A catalog identifier. Defaults to 'common'. output (bool, optional): If True then print the dataframe. Defaults to False. Returns: pandas.DataFrame: A dataframe with a row for each resource \"\"\" catalog = self . __use_catalog ( catalog ) url = f ' { self . root_url } catalogs/ { catalog } /datasets/ { dataset } ' df = Fusion . _call_for_dataframe ( url , self . session ) if output : print ( tabulate ( df , headers = \"keys\" , tablefmt = \"psql\" )) return df datasetmember_resources ( self , dataset , series , catalog = None , output = False ) \u00b6 List the available resources for a datasetseries member. Parameters: Name Type Description Default dataset str A dataset identifier required series str The datasetseries identifier required catalog str A catalog identifier. Defaults to 'common'. None output bool If True then print the dataframe. Defaults to False. False Returns: Type Description DataFrame pandas.DataFrame: A dataframe with a row for each datasetseries member resource. Currently, this will always be distributions. Source code in fusion/fusion.py def datasetmember_resources ( self , dataset : str , series : str , catalog : str = None , output : bool = False ) -> pd . DataFrame : \"\"\"List the available resources for a datasetseries member. Args: dataset (str): A dataset identifier series (str): The datasetseries identifier catalog (str, optional): A catalog identifier. Defaults to 'common'. output (bool, optional): If True then print the dataframe. Defaults to False. Returns: pandas.DataFrame: A dataframe with a row for each datasetseries member resource. Currently, this will always be distributions. \"\"\" catalog = self . __use_catalog ( catalog ) url = f ' { self . root_url } catalogs/ { catalog } /datasets/ { dataset } /datasetseries/ { series } ' df = Fusion . _call_for_dataframe ( url , self . session ) if output : print ( tabulate ( df , headers = \"keys\" , tablefmt = \"psql\" )) return df download ( self , dataset , dt_str = 'latest' , dataset_format = 'parquet' , catalog = None , n_par = None , show_progress = True , force_download = False , download_folder = None , return_paths = False ) \u00b6 Downloads the requested distributions of a dataset to disk. Parameters: Name Type Description Default dataset str A dataset identifier required dt_str str Either a single date or a range identified by a start or end date, or both separated with a \":\". Defaults to 'latest' which will return the most recent instance of the dataset. 'latest' dataset_format str The file format, e.g. CSV or Parquet. Defaults to 'parquet'. 'parquet' catalog str A catalog identifier. Defaults to 'common'. None n_par int Specify how many distributions to download in parallel. Defaults to all cpus available. None show_progress bool Display a progress bar during data download Defaults to True. True force_download bool If True then will always download a file even if it is already on disk. Defaults to True. False download_folder str The path, absolute or relative, where downloaded files are saved. Defaults to download_folder as set in init None return_paths bool Return paths and success statuses of the downloaded files. False Source code in fusion/fusion.py def download ( self , dataset : str , dt_str : str = 'latest' , dataset_format : str = 'parquet' , catalog : str = None , n_par : int = None , show_progress : bool = True , force_download : bool = False , download_folder : str = None , return_paths : bool = False , ): \"\"\"Downloads the requested distributions of a dataset to disk. Args: dataset (str): A dataset identifier dt_str (str, optional): Either a single date or a range identified by a start or end date, or both separated with a \":\". Defaults to 'latest' which will return the most recent instance of the dataset. dataset_format (str, optional): The file format, e.g. CSV or Parquet. Defaults to 'parquet'. catalog (str, optional): A catalog identifier. Defaults to 'common'. n_par (int, optional): Specify how many distributions to download in parallel. Defaults to all cpus available. show_progress (bool, optional): Display a progress bar during data download Defaults to True. force_download (bool, optional): If True then will always download a file even if it is already on disk. Defaults to True. download_folder (str, optional): The path, absolute or relative, where downloaded files are saved. Defaults to download_folder as set in __init__ return_paths (bool, optional): Return paths and success statuses of the downloaded files. Returns: \"\"\" catalog = self . __use_catalog ( catalog ) n_par = cpu_count ( n_par ) required_series = self . _resolve_distro_tuples ( dataset , dt_str , dataset_format , catalog ) if not download_folder : download_folder = self . download_folder if not self . fs . exists ( download_folder ): self . fs . mkdir ( download_folder , create_parents = True ) download_spec = [ { \"credentials\" : self . credentials , \"url\" : distribution_to_url ( self . root_url , series [ 1 ], series [ 2 ], series [ 3 ], series [ 0 ]), \"output_file\" : distribution_to_filename ( download_folder , series [ 1 ], series [ 2 ], series [ 3 ], series [ 0 ]), \"overwrite\" : force_download , \"fs\" : self . fs } for series in required_series ] if show_progress : loop = tqdm ( download_spec ) else : loop = download_spec logger . log ( VERBOSE_LVL , f 'Beginning { len ( loop ) } downloads in batches of { n_par } ' , ) res = Parallel ( n_jobs = n_par )( delayed ( stream_single_file_new_session )( ** spec ) for spec in loop ) return res if return_paths else None get_fusion_filesystem ( self ) \u00b6 Creates Fusion Filesystem. Returns: Fusion Filesystem Source code in fusion/fusion.py def get_fusion_filesystem ( self ): \"\"\"Creates Fusion Filesystem. Returns: Fusion Filesystem \"\"\" return FusionHTTPFileSystem ( client_kwargs = { \"root_url\" : self . root_url , \"credentials\" : self . credentials }) list_catalogs ( self , output = False ) \u00b6 Lists the catalogs available to the API account. Parameters: Name Type Description Default output bool If True then print the dataframe. Defaults to False. False Returns: Type Description DataFrame pandas.DataFrame: A dataframe with a row for each catalog Source code in fusion/fusion.py def list_catalogs ( self , output : bool = False ) -> pd . DataFrame : \"\"\"Lists the catalogs available to the API account. Args: output (bool, optional): If True then print the dataframe. Defaults to False. Returns: pandas.DataFrame: A dataframe with a row for each catalog \"\"\" url = f ' { self . root_url } catalogs/' df = Fusion . _call_for_dataframe ( url , self . session ) if output : print ( tabulate ( df , headers = \"keys\" , tablefmt = \"psql\" )) return df list_dataset_attributes ( self , dataset , catalog = None , output = False , display_all_columns = False ) \u00b6 Returns the list of attributes that are in the dataset. Parameters: Name Type Description Default dataset str A dataset identifier required catalog str A catalog identifier. Defaults to 'common'. None output bool If True then print the dataframe. Defaults to False. False display_all_columns bool If True displays all columns returned by the API, otherwise only the key columns are displayed False Returns: Type Description DataFrame pandas.DataFrame: A dataframe with a row for each attribute Source code in fusion/fusion.py def list_dataset_attributes ( self , dataset : str , catalog : str = None , output : bool = False , display_all_columns : bool = False ) -> pd . DataFrame : \"\"\"Returns the list of attributes that are in the dataset. Args: dataset (str): A dataset identifier catalog (str, optional): A catalog identifier. Defaults to 'common'. output (bool, optional): If True then print the dataframe. Defaults to False. display_all_columns (bool, optional): If True displays all columns returned by the API, otherwise only the key columns are displayed Returns: pandas.DataFrame: A dataframe with a row for each attribute \"\"\" catalog = self . __use_catalog ( catalog ) url = f ' { self . root_url } catalogs/ { catalog } /datasets/ { dataset } /attributes' df = Fusion . _call_for_dataframe ( url , self . session ) if not display_all_columns : df = df [[ \"identifier\" , \"dataType\" , \"isDatasetKey\" , \"description\" ]] if output : print ( tabulate ( df , headers = \"keys\" , tablefmt = \"psql\" , maxcolwidths = 30 )) return df list_datasetmembers ( self , dataset , catalog = None , output = False , max_results =- 1 ) \u00b6 List the available members in the dataset series. Parameters: Name Type Description Default dataset str A dataset identifier required catalog str A catalog identifier. Defaults to 'common'. None output bool If True then print the dataframe. Defaults to False. False max_results int Limit the number of rows returned in the dataframe. Defaults to -1 which returns all results. -1 Returns: Type Description DataFrame pandas.DataFrame: a dataframe with a row for each dataset member. Source code in fusion/fusion.py def list_datasetmembers ( self , dataset : str , catalog : str = None , output : bool = False , max_results : int = - 1 ) -> pd . DataFrame : \"\"\"List the available members in the dataset series. Args: dataset (str): A dataset identifier catalog (str, optional): A catalog identifier. Defaults to 'common'. output (bool, optional): If True then print the dataframe. Defaults to False. max_results (int, optional): Limit the number of rows returned in the dataframe. Defaults to -1 which returns all results. Returns: pandas.DataFrame: a dataframe with a row for each dataset member. \"\"\" catalog = self . __use_catalog ( catalog ) url = f ' { self . root_url } catalogs/ { catalog } /datasets/ { dataset } /datasetseries' df = Fusion . _call_for_dataframe ( url , self . session ) if max_results > - 1 : df = df [ 0 : max_results ] if output : print ( tabulate ( df , headers = \"keys\" , tablefmt = \"psql\" )) return df list_datasets ( self , contains = None , id_contains = False , catalog = None , output = False , max_results =- 1 , display_all_columns = False ) \u00b6 summary . Parameters: Name Type Description Default contains Union[str, list] A string or a list of strings that are dataset identifiers to filter the datasets list. If a list is provided then it will return datasets whose identifier matches any of the strings. Defaults to None. None id_contains bool Filter datasets only where the string(s) are contained in the identifier, ignoring description. False catalog str A catalog identifier. Defaults to 'common'. None output bool If True then print the dataframe. Defaults to False. False max_results int Limit the number of rows returned in the dataframe. Defaults to -1 which returns all results. -1 display_all_columns bool If True displays all columns returned by the API, otherwise only the key columns are displayed False Returns: Type Description DataFrame pandas.DataFrame: a dataframe with a row for each dataset. Source code in fusion/fusion.py def list_datasets ( self , contains : Union [ str , list ] = None , id_contains : bool = False , catalog : str = None , output : bool = False , max_results : int = - 1 , display_all_columns : bool = False , ) -> pd . DataFrame : \"\"\"_summary_. Args: contains (Union[str, list], optional): A string or a list of strings that are dataset identifiers to filter the datasets list. If a list is provided then it will return datasets whose identifier matches any of the strings. Defaults to None. id_contains (bool): Filter datasets only where the string(s) are contained in the identifier, ignoring description. catalog (str, optional): A catalog identifier. Defaults to 'common'. output (bool, optional): If True then print the dataframe. Defaults to False. max_results (int, optional): Limit the number of rows returned in the dataframe. Defaults to -1 which returns all results. display_all_columns (bool, optional): If True displays all columns returned by the API, otherwise only the key columns are displayed Returns: pandas.DataFrame: a dataframe with a row for each dataset. \"\"\" catalog = self . __use_catalog ( catalog ) url = f ' { self . root_url } catalogs/ { catalog } /datasets' df = Fusion . _call_for_dataframe ( url , self . session ) if contains : if isinstance ( contains , list ): contains = \"|\" . join ( f ' { s } ' for s in contains ) if id_contains : df = df [ df [ 'identifier' ] . str . contains ( contains , case = False )] else : df = df [ df [ 'identifier' ] . str . contains ( contains , case = False ) | df [ 'description' ] . str . contains ( contains , case = False ) ] if max_results > - 1 : df = df [ 0 : max_results ] df [ \"category\" ] = df . category . str . join ( \", \" ) df [ \"region\" ] = df . region . str . join ( \", \" ) if not display_all_columns : df = df [ [ \"identifier\" , \"title\" , \"region\" , \"category\" , \"coverageStartDate\" , \"coverageEndDate\" , \"description\" ] ] if output : print ( tabulate ( df , headers = \"keys\" , tablefmt = \"psql\" , maxcolwidths = 30 )) return df list_distributions ( self , dataset , series , catalog = None , output = False ) \u00b6 List the available distributions (downloadable instances of the dataset with a format type). Parameters: Name Type Description Default dataset str A dataset identifier required series str The datasetseries identifier required catalog str A catalog identifier. Defaults to 'common'. None output bool If True then print the dataframe. Defaults to False. False Returns: Type Description DataFrame pandas.DataFrame: A dataframe with a row for each distribution. Source code in fusion/fusion.py def list_distributions ( self , dataset : str , series : str , catalog : str = None , output : bool = False ) -> pd . DataFrame : \"\"\"List the available distributions (downloadable instances of the dataset with a format type). Args: dataset (str): A dataset identifier series (str): The datasetseries identifier catalog (str, optional): A catalog identifier. Defaults to 'common'. output (bool, optional): If True then print the dataframe. Defaults to False. Returns: pandas.DataFrame: A dataframe with a row for each distribution. \"\"\" catalog = self . __use_catalog ( catalog ) url = f ' { self . root_url } catalogs/ { catalog } /datasets/ { dataset } /datasetseries/ { series } /distributions' df = Fusion . _call_for_dataframe ( url , self . session ) if output : print ( tabulate ( df , headers = \"keys\" , tablefmt = \"psql\" )) return df list_products ( self , contains = None , id_contains = False , catalog = None , output = False , max_results =- 1 , display_all_columns = False ) \u00b6 Get the products contained in a catalog. A product is a grouping of datasets. Parameters: Name Type Description Default contains Union[str, list] A string or a list of strings that are product identifiers to filter the products list. If a list is provided then it will return products whose identifier matches any of the strings. Defaults to None. None id_contains bool Filter datasets only where the string(s) are contained in the identifier, ignoring description. False catalog str A catalog identifier. Defaults to 'common'. None output bool If True then print the dataframe. Defaults to False. False max_results int Limit the number of rows returned in the dataframe. Defaults to -1 which returns all results. -1 display_all_columns bool If True displays all columns returned by the API, otherwise only the key columns are displayed False Returns: Type Description DataFrame pandas.DataFrame: a dataframe with a row for each product Source code in fusion/fusion.py def list_products ( self , contains : Union [ str , list ] = None , id_contains : bool = False , catalog : str = None , output : bool = False , max_results : int = - 1 , display_all_columns : bool = False , ) -> pd . DataFrame : \"\"\"Get the products contained in a catalog. A product is a grouping of datasets. Args: contains (Union[str, list], optional): A string or a list of strings that are product identifiers to filter the products list. If a list is provided then it will return products whose identifier matches any of the strings. Defaults to None. id_contains (bool): Filter datasets only where the string(s) are contained in the identifier, ignoring description. catalog (str, optional): A catalog identifier. Defaults to 'common'. output (bool, optional): If True then print the dataframe. Defaults to False. max_results (int, optional): Limit the number of rows returned in the dataframe. Defaults to -1 which returns all results. display_all_columns (bool, optional): If True displays all columns returned by the API, otherwise only the key columns are displayed Returns: pandas.DataFrame: a dataframe with a row for each product \"\"\" catalog = self . __use_catalog ( catalog ) url = f ' { self . root_url } catalogs/ { catalog } /products' df = Fusion . _call_for_dataframe ( url , self . session ) if contains : if isinstance ( contains , list ): contains = \"|\" . join ( f ' { s } ' for s in contains ) if id_contains : df = df [ df [ 'identifier' ] . str . contains ( contains , case = False )] else : df = df [ df [ 'identifier' ] . str . contains ( contains , case = False ) | df [ 'description' ] . str . contains ( contains , case = False ) ] df [ \"category\" ] = df . category . str . join ( \", \" ) df [ \"region\" ] = df . region . str . join ( \", \" ) if not display_all_columns : df = df [[ \"identifier\" , \"title\" , \"region\" , \"category\" , \"status\" , \"description\" ]] if max_results > - 1 : df = df [ 0 : max_results ] if output : print ( tabulate ( df , headers = \"keys\" , tablefmt = \"psql\" , maxcolwidths = 30 )) return df to_df ( self , dataset , dt_str = 'latest' , dataset_format = 'parquet' , catalog = None , n_par = None , show_progress = True , columns = None , filters = None , force_download = False , download_folder = None , ** kwargs ) \u00b6 Gets distributions for a specified date or date range and returns the data as a dataframe. Parameters: Name Type Description Default dataset str A dataset identifier required dt_str str Either a single date or a range identified by a start or end date, or both separated with a \":\". Defaults to 'latest' which will return the most recent instance of the dataset. 'latest' dataset_format str The file format, e.g. CSV or Parquet. Defaults to 'parquet'. 'parquet' catalog str A catalog identifier. Defaults to 'common'. None n_par int Specify how many distributions to download in parallel. Defaults to all cpus available. None show_progress bool Display a progress bar during data download Defaults to True. True columns List A list of columns to return from a parquet file. Defaults to None None filters List List[Tuple] or List[List[Tuple]] or None (default) Rows which do not match the filter predicate will be removed from scanned data. Partition keys embedded in a nested directory structure will be exploited to avoid loading files at all if they contain no matching rows. If use_legacy_dataset is True, filters can only reference partition keys and only a hive-style directory structure is supported. When setting use_legacy_dataset to False, also within-file level filtering and different partitioning schemes are supported. More on https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetDataset.html None force_download bool If True then will always download a file even if it is already on disk. Defaults to False. False download_folder str The path, absolute or relative, where downloaded files are saved. Defaults to download_folder as set in init None Returns: Type Description DataFrame pandas.DataFrame: a dataframe containing the requested data. If multiple dataset instances are retrieved then these are concatenated first. Source code in fusion/fusion.py def to_df ( self , dataset : str , dt_str : str = 'latest' , dataset_format : str = 'parquet' , catalog : str = None , n_par : int = None , show_progress : bool = True , columns : List = None , filters : List = None , force_download : bool = False , download_folder : str = None , ** kwargs , ) -> pd . DataFrame : \"\"\"Gets distributions for a specified date or date range and returns the data as a dataframe. Args: dataset (str): A dataset identifier dt_str (str, optional): Either a single date or a range identified by a start or end date, or both separated with a \":\". Defaults to 'latest' which will return the most recent instance of the dataset. dataset_format (str, optional): The file format, e.g. CSV or Parquet. Defaults to 'parquet'. catalog (str, optional): A catalog identifier. Defaults to 'common'. n_par (int, optional): Specify how many distributions to download in parallel. Defaults to all cpus available. show_progress (bool, optional): Display a progress bar during data download Defaults to True. columns (List, optional): A list of columns to return from a parquet file. Defaults to None filters (List, optional): List[Tuple] or List[List[Tuple]] or None (default) Rows which do not match the filter predicate will be removed from scanned data. Partition keys embedded in a nested directory structure will be exploited to avoid loading files at all if they contain no matching rows. If use_legacy_dataset is True, filters can only reference partition keys and only a hive-style directory structure is supported. When setting use_legacy_dataset to False, also within-file level filtering and different partitioning schemes are supported. More on https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetDataset.html force_download (bool, optional): If True then will always download a file even if it is already on disk. Defaults to False. download_folder (str, optional): The path, absolute or relative, where downloaded files are saved. Defaults to download_folder as set in __init__ Returns: pandas.DataFrame: a dataframe containing the requested data. If multiple dataset instances are retrieved then these are concatenated first. \"\"\" catalog = self . __use_catalog ( catalog ) n_par = cpu_count ( n_par ) if not download_folder : download_folder = self . download_folder download_res = self . download ( dataset , dt_str , dataset_format , catalog , n_par , show_progress , force_download , download_folder , return_paths = True ) if not all ( res [ 0 ] for res in download_res ): failed_res = [ res for res in download_res if not res [ 0 ]] raise Exception ( f \"Not all downloads were successfully completed. \" f \"Re-run to collect missing files. The following failed: \\n { failed_res } \" ) files = [ res [ 1 ] for res in download_res ] pd_read_fn_map = { 'csv' : read_csv , 'parquet' : read_parquet , 'parq' : read_parquet , 'json' : pd . read_json , } pd_read_default_kwargs : Dict [ str , Dict [ str , object ]] = { 'csv' : { 'columns' : columns , 'filters' : filters }, 'parquet' : { 'columns' : columns , 'filters' : filters }, 'json' : { 'columns' : columns , 'filters' : filters }, } pd_read_default_kwargs [ 'parq' ] = pd_read_default_kwargs [ 'parquet' ] pd_reader = pd_read_fn_map . get ( dataset_format ) pd_read_kwargs = pd_read_default_kwargs . get ( dataset_format , {}) if not pd_reader : raise Exception ( f 'No pandas function to read file in format { dataset_format } ' ) pd_read_kwargs . update ( kwargs ) if len ( files ) == 0 : raise APIResponseError ( f \"No series members for dataset: { dataset } \" f \"in date or date range: { dt_str } and format: { dataset_format } \" ) if dataset_format in [ \"parquet\" , \"parq\" ]: df = pd_reader ( files , ** pd_read_kwargs ) else : dataframes = ( pd_reader ( f , ** pd_read_kwargs ) for f in files ) df = pd . concat ( dataframes , ignore_index = True ) return df upload ( self , path , dataset = None , dt_str = 'latest' , catalog = None , n_par = None , show_progress = True , return_paths = False ) \u00b6 Uploads the requested files/files to Fusion. Parameters: Name Type Description Default path str path to a file or a folder with files required dataset str Dataset name to which the file will be uplaoded (for single file only). If not provided the dataset will be implied from file's name. None dt_str str A single date. Defaults to 'latest' which will return the most recent. Relevant for a single file upload only. If not provided the dataset will be implied from file's name. 'latest' catalog str A catalog identifier. Defaults to 'common'. None n_par int Specify how many distributions to download in parallel. Defaults to all cpus available. None show_progress bool Display a progress bar during data download Defaults to True. True return_paths bool Return paths and success statuses of the downloaded files. False Source code in fusion/fusion.py def upload ( self , path : str , dataset : str = None , dt_str : str = 'latest' , catalog : str = None , n_par : int = None , show_progress : bool = True , return_paths : bool = False , ): \"\"\"Uploads the requested files/files to Fusion. Args: path (str): path to a file or a folder with files dataset (str, optional): Dataset name to which the file will be uplaoded (for single file only). If not provided the dataset will be implied from file's name. dt_str (str, optional): A single date. Defaults to 'latest' which will return the most recent. Relevant for a single file upload only. If not provided the dataset will be implied from file's name. catalog (str, optional): A catalog identifier. Defaults to 'common'. n_par (int, optional): Specify how many distributions to download in parallel. Defaults to all cpus available. show_progress (bool, optional): Display a progress bar during data download Defaults to True. return_paths (bool, optional): Return paths and success statuses of the downloaded files. Returns: \"\"\" catalog = self . __use_catalog ( catalog ) if not self . fs . exists ( path ): raise RuntimeError ( \"The provided path does not exist\" ) fs_fusion = self . get_fusion_filesystem () if self . fs . info ( path )[ \"type\" ] == \"directory\" : file_path_lst = self . fs . find ( path ) local_file_validation = validate_file_names ( file_path_lst , fs_fusion ) file_path_lst = [ f for flag , f in zip ( local_file_validation , file_path_lst ) if flag ] local_url_eqiv = [ path_to_url ( i ) for i in file_path_lst ] else : file_path_lst = [ path ] if not catalog or not dataset : local_file_validation = validate_file_names ( file_path_lst , fs_fusion ) file_path_lst = [ f for flag , f in zip ( local_file_validation , file_path_lst ) if flag ] local_url_eqiv = [ path_to_url ( i ) for i in file_path_lst ] else : file_format = path . split ( \".\" )[ - 1 ] dt_str = dt_str if dt_str != \"latest\" else pd . Timestamp ( \"today\" ) . date () . strftime ( \"%Y%m %d \" ) dt_str = pd . Timestamp ( dt_str ) . date () . strftime ( \"%Y%m %d \" ) if catalog not in fs_fusion . ls ( '' ) or dataset not in [ i . split ( '/' )[ - 1 ] for i in fs_fusion . ls ( f \" { catalog } /datasets\" )]: msg = f \"File file has not been uploaded, one of the catalog: { catalog } \" \\ f \"or dataset: { dataset } does not exit.\" warnings . warn ( msg ) return [( False , path , Exception ( msg ))] local_url_eqiv = [ path_to_url ( f \" { dataset } __ { catalog } __ { dt_str } . { file_format } \" )] df = pd . DataFrame ([ file_path_lst , local_url_eqiv ]) . T df . columns = [ \"path\" , \"url\" ] if show_progress : loop = tqdm ( df . iterrows (), total = len ( df )) else : loop = df . iterrows () n_par = cpu_count ( n_par ) parallel = True if len ( df ) > 1 else False res = upload_files ( fs_fusion , self . fs , loop , parallel = parallel , n_par = n_par ) if not all ( r [ 0 ] for r in res ): failed_res = [ r for r in res if not r [ 0 ]] msg = f \"Not all uploads were successfully completed. The following failed: \\n { failed_res } \" logger . warning ( msg ) warnings . warn ( msg ) return res if return_paths else None fusion_filesystem \u00b6 Fusion FileSystem. FusionHTTPFileSystem \u00b6 Fusion HTTP filesystem. __init__ ( self , credentials = 'config/client_credentials.json' , * args , ** kwargs ) special \u00b6 Same signature as the fsspec HTTPFileSystem. Parameters: Name Type Description Default credentials Credentials. 'config/client_credentials.json' *args Args. () **kwargs Kwargs. {} Source code in fusion/fusion_filesystem.py def __init__ ( self , credentials = 'config/client_credentials.json' , * args , ** kwargs ): \"\"\"Same signature as the fsspec HTTPFileSystem. Args: credentials: Credentials. *args: Args. **kwargs: Kwargs. \"\"\" self . credentials = credentials if \"get_client\" not in kwargs : kwargs [ \"get_client\" ] = get_client if \"client_kwargs\" not in kwargs : if isinstance ( credentials , FusionCredentials ): self . credentials = credentials else : self . credentials = FusionCredentials . from_object ( credentials ) kwargs [ \"client_kwargs\" ] = { \"credentials\" : self . credentials , \"root_url\" : \"https://fusion-api.jpmorgan.com/fusion/v1/\" } if self . credentials . proxies : if \"http\" in self . credentials . proxies . keys (): kwargs [ \"proxy\" ] = self . credentials . proxies [ \"http\" ] elif \"https\" in self . credentials . proxies . keys (): kwargs [ \"proxy\" ] = self . credentials . proxies [ \"https\" ] if \"headers\" not in kwargs : kwargs [ \"headers\" ] = { \"Accept-Encoding\" : \"identity\" } super () . __init__ ( * args , ** kwargs ) cat ( self , url , start = None , end = None , ** kwargs ) \u00b6 Fetch paths' contents. Parameters: Name Type Description Default url Url. required start Start. None end End. None **kwargs Kwargs. {} Source code in fusion/fusion_filesystem.py def cat ( self , url , start = None , end = None , ** kwargs ): \"\"\"Fetch paths' contents. Args: url: Url. start: Start. end: End. **kwargs: Kwargs. Returns: \"\"\" url = self . _decorate_url ( url ) return super () . cat ( url , start = start , end = end , ** kwargs ) exists ( self , url , detail = True , ** kwargs ) \u00b6 Check existence. Parameters: Name Type Description Default url Url. required detail Detail. True **kwargs Kwargs. {} Source code in fusion/fusion_filesystem.py def exists ( self , url , detail = True , ** kwargs ): \"\"\"Check existence. Args: url: Url. detail: Detail. **kwargs: Kwargs. Returns: \"\"\" url = self . _decorate_url ( url ) return super () . exists ( url , ** kwargs ) find ( self , path , maxdepth = None , withdirs = False , ** kwargs ) \u00b6 Find all file in a folder. Parameters: Name Type Description Default path Path. required maxdepth Max depth. None withdirs With dirs, default to False. False **kwargs Kwargs. {} Source code in fusion/fusion_filesystem.py def find ( self , path , maxdepth = None , withdirs = False , ** kwargs ): \"\"\"Find all file in a folder. Args: path: Path. maxdepth: Max depth. withdirs: With dirs, default to False. **kwargs: Kwargs. Returns: \"\"\" path = self . _decorate_url ( path ) return super () . find ( path , maxdepth = maxdepth , withdirs = withdirs , ** kwargs ) get ( self , rpath , lpath , chunk_size = 5242880 , callback =< fsspec . callbacks . NoOpCallback object at 0x7f6ab1c21e50 > , ** kwargs ) \u00b6 Copy file(s) to local. Parameters: Name Type Description Default rpath Rpath. required lpath Lpath. required chunk_size Chunk size. 5242880 callback Callback function. <fsspec.callbacks.NoOpCallback object at 0x7f6ab1c21e50> **kwargs Kwargs. {} Source code in fusion/fusion_filesystem.py def get ( self , rpath , lpath , chunk_size = 5 * 2 ** 20 , callback = _DEFAULT_CALLBACK , ** kwargs ): \"\"\"Copy file(s) to local. Args: rpath: Rpath. lpath: Lpath. chunk_size: Chunk size. callback: Callback function. **kwargs: Kwargs. Returns: \"\"\" rpath = self . _decorate_url ( rpath ) return super () . get ( rpath , lpath , chunk_size = 5 * 2 ** 20 , callback = _DEFAULT_CALLBACK , ** kwargs ) glob ( self , path , ** kwargs ) \u00b6 Glob. Parameters: Name Type Description Default path Path. required **kwargs Kwargs. {} Source code in fusion/fusion_filesystem.py def glob ( self , path , ** kwargs ): \"\"\"Glob. Args: path: Path. **kwargs: Kwargs. Returns: \"\"\" return super () . glob ( path , ** kwargs ) info ( self , path , ** kwargs ) \u00b6 Return info. Parameters: Name Type Description Default path Path. required **kwargs Kwargs. {} Source code in fusion/fusion_filesystem.py def info ( self , path , ** kwargs ): \"\"\"Return info. Args: path: Path. **kwargs: Kwargs. Returns: \"\"\" path = self . _decorate_url ( path ) kwargs [ \"keep_protocol\" ] = True res = super () . ls ( path , detail = True , ** kwargs )[ 0 ] if res [ \"type\" ] == \"file\" : return res else : return super () . info ( path , ** kwargs ) isfile ( self , path ) \u00b6 Is path a file. Parameters: Name Type Description Default path Path. required Source code in fusion/fusion_filesystem.py def isfile ( self , path ): \"\"\"Is path a file. Args: path: Path. Returns: \"\"\" path = self . _decorate_url ( path ) return super () . isfile ( path ) ls ( self , url , detail = False , ** kwargs ) \u00b6 List resources. Parameters: Name Type Description Default url Url. required detail Detail. False **kwargs Kwargs. {} Source code in fusion/fusion_filesystem.py def ls ( self , url , detail = False , ** kwargs ): \"\"\"List resources. Args: url: Url. detail: Detail. **kwargs: Kwargs. Returns: \"\"\" url = self . _decorate_url ( url ) ret = super () . ls ( url , detail = detail , ** kwargs ) keep_protocol = kwargs . pop ( \"keep_protocol\" , False ) if detail : if not keep_protocol : for k in ret : k [ \"name\" ] = k [ \"name\" ] . split ( f ' { self . client_kwargs [ \"root_url\" ] } catalogs/' )[ - 1 ] else : if not keep_protocol : return [ x . split ( f ' { self . client_kwargs [ \"root_url\" ] } catalogs/' )[ - 1 ] for x in ret ] return ret open ( self , path , mode = 'rb' , ** kwargs ) \u00b6 Open. Parameters: Name Type Description Default path Path. required mode Defaults to rb. 'rb' **kwargs Kwargs. {} Source code in fusion/fusion_filesystem.py def open ( self , path , mode = \"rb\" , ** kwargs , ): \"\"\"Open. Args: path: Path. mode: Defaults to rb. **kwargs: Kwargs. Returns: \"\"\" path = self . _decorate_url ( path ) return super () . open ( path , mode , ** kwargs ) put ( self , lpath , rpath , chunk_size = 5242880 , callback =< fsspec . callbacks . NoOpCallback object at 0x7f6ab1c21e50 > , method = 'put' , ** kwargs ) \u00b6 Copy file(s) from local. Parameters: Name Type Description Default lpath Lpath. required rpath Rpath. required chunk_size Chunk size. 5242880 callback Callback function. <fsspec.callbacks.NoOpCallback object at 0x7f6ab1c21e50> method 'put' **kwargs Kwargs. {} Source code in fusion/fusion_filesystem.py def put ( self , lpath , rpath , chunk_size = 5 * 2 ** 20 , callback = _DEFAULT_CALLBACK , method = \"put\" , ** kwargs ): \"\"\"Copy file(s) from local. Args: lpath: Lpath. rpath: Rpath. chunk_size: Chunk size. callback: Callback function. method: Method: put/post. **kwargs: Kwargs. Returns: \"\"\" rpath = self . _decorate_url ( rpath ) args = [ lpath , rpath , chunk_size , callback , method ] return sync ( super () . loop , super () . _put_file , * args , ** kwargs ) utils \u00b6 Fusion utilities. cpu_count ( thread_pool_size = None ) \u00b6 Determine the number of cpus/threads for parallelization. Parameters: Name Type Description Default thread_pool_size int override argument for number of cpus/threads. None Source code in fusion/utils.py def cpu_count ( thread_pool_size : int = None ): \"\"\"Determine the number of cpus/threads for parallelization. Args: thread_pool_size: override argument for number of cpus/threads. Returns: number of cpus/threads to use. \"\"\" if os . environ . get ( \"NUM_THREADS\" ) is not None : return int ( os . environ [ \"NUM_THREADS\" ]) if thread_pool_size : return thread_pool_size else : if mp . cpu_count (): thread_pool_size = mp . cpu_count () else : thread_pool_size = DEFAULT_THREAD_POOL_SIZE return thread_pool_size distribution_to_filename ( root_folder , dataset , datasetseries , file_format , catalog = 'common' ) \u00b6 Returns a filename representing a dataset distribution. Parameters: Name Type Description Default root_folder str The root folder path. required dataset str The dataset identifier. required datasetseries str The datasetseries instance identifier. required file_format str The file type, e.g. CSV or Parquet required catalog str The data catalog containing the dataset. Defaults to \"common\". 'common' Returns: Type Description Path Source code in fusion/utils.py def distribution_to_filename ( root_folder : str , dataset : str , datasetseries : str , file_format : str , catalog : str = 'common' ) -> Path : \"\"\"Returns a filename representing a dataset distribution. Args: root_folder (str): The root folder path. dataset (str): The dataset identifier. datasetseries (str): The datasetseries instance identifier. file_format (str): The file type, e.g. CSV or Parquet catalog (str, optional): The data catalog containing the dataset. Defaults to \"common\". Returns: tuple: A tuple of dates. \"\"\" if datasetseries [ - 1 ] == '/' or datasetseries [ - 1 ] == ' \\\\ ' : datasetseries = datasetseries [ 0 : - 1 ] file_name = f \" { dataset } __ { catalog } __ { datasetseries } . { file_format } \" return Path ( root_folder , file_name ) distribution_to_url ( root_url , dataset , datasetseries , file_format , catalog = 'common' ) \u00b6 Returns the API URL to download a dataset distribution. Parameters: Name Type Description Default root_url str The base url for the API. required dataset str The dataset identifier. required datasetseries str The datasetseries instance identifier. required file_format str The file type, e.g. CSV or Parquet required catalog str The data catalog containing the dataset. Defaults to \"common\". 'common' Returns: Type Description str Source code in fusion/utils.py def distribution_to_url ( root_url : str , dataset : str , datasetseries : str , file_format : str , catalog : str = 'common' ) -> str : \"\"\"Returns the API URL to download a dataset distribution. Args: root_url (str): The base url for the API. dataset (str): The dataset identifier. datasetseries (str): The datasetseries instance identifier. file_format (str): The file type, e.g. CSV or Parquet catalog (str, optional): The data catalog containing the dataset. Defaults to \"common\". Returns: str: A URL for the API distribution endpoint. \"\"\" if datasetseries [ - 1 ] == '/' or datasetseries [ - 1 ] == ' \\\\ ' : datasetseries = datasetseries [ 0 : - 1 ] return f \" { root_url } catalogs/ { catalog } /datasets/ { dataset } /datasetseries/ { datasetseries } /distributions/ { file_format } \" get_client ( credentials , ** kwargs ) async \u00b6 Gets session for async. Parameters: Name Type Description Default credentials Credentials. required **kwargs Kwargs. {} Source code in fusion/utils.py async def get_client ( credentials , ** kwargs ): \"\"\"Gets session for async. Args: credentials: Credentials. **kwargs: Kwargs. Returns: \"\"\" async def on_request_start ( session , trace_config_ctx , params ): payload = ( { \"grant_type\" : \"client_credentials\" , \"client_id\" : credentials . client_id , \"client_secret\" : credentials . client_secret , \"aud\" : credentials . resource , } if credentials . grant_type == 'client_credentials' else { \"grant_type\" : \"password\" , \"client_id\" : credentials . client_id , \"username\" : credentials . username , \"password\" : credentials . password , \"resource\" : credentials . resource , } ) async with aiohttp . ClientSession () as session : if credentials . proxies : response = await session . post ( credentials . auth_url , data = payload , proxy = http_proxy ) else : response = await session . post ( credentials . auth_url , data = payload ) response_data = await response . json () access_token = response_data [ \"access_token\" ] params . headers . update ({ 'Authorization' : f 'Bearer { access_token } ' }) if credentials . proxies : if \"http\" in credentials . proxies . keys (): http_proxy = credentials . proxies [ \"http\" ] elif \"https\" in credentials . proxies . keys (): http_proxy = credentials . proxies [ \"https\" ] trace_config = aiohttp . TraceConfig () trace_config . on_request_start . append ( on_request_start ) return aiohttp . ClientSession ( trace_configs = [ trace_config ], trust_env = True ) get_session ( credentials , root_url , get_retries = None ) \u00b6 Create a new http session and set parameters. Parameters: Name Type Description Default credentials FusionCredentials Valid user credentials to provide an acces token required root_url str The URL to call. required Returns: Type Description Session requests.Session(): A HTTP Session object Source code in fusion/utils.py def get_session ( credentials : FusionCredentials , root_url : str , get_retries : Union [ int , Retry ] = None ) -> requests . Session : \"\"\"Create a new http session and set parameters. Args: credentials (FusionCredentials): Valid user credentials to provide an acces token root_url (str): The URL to call. Returns: requests.Session(): A HTTP Session object \"\"\" if not get_retries : get_retries = Retry ( total = 5 , backoff_factor = 0.1 , status_forcelist = [ 429 , 500 , 502 , 503 , 504 ]) else : get_retries = Retry . from_int ( get_retries ) session = requests . Session () auth_handler = FusionOAuthAdapter ( credentials , max_retries = get_retries ) if credentials . proxies : # mypy does note recognise session.proxies as a dict so fails this line, we'll ignore this chk session . proxies . update ( credentials . proxies ) # type:ignore try : mount_url = _get_canonical_root_url ( root_url ) except Exception : mount_url = \"https://\" session . mount ( mount_url , auth_handler ) return session normalise_dt_param_str ( dt ) \u00b6 Convert a date parameter which may be a single date or a date range into a tuple. Parameters: Name Type Description Default dt str Either a single date or a date range separated by a \":\". required Returns: Type Description tuple Source code in fusion/utils.py def normalise_dt_param_str ( dt : str ) -> tuple : \"\"\"Convert a date parameter which may be a single date or a date range into a tuple. Args: dt (str): Either a single date or a date range separated by a \":\". Returns: tuple: A tuple of dates. \"\"\" date_parts = dt . split ( \":\" ) if not date_parts or len ( date_parts ) > 2 : raise ValueError ( f \"Unable to parse { dt } as either a date or an interval\" ) return tuple (( _normalise_dt_param ( dt_part ) if dt_part else dt_part for dt_part in date_parts )) path_to_url ( x ) \u00b6 Convert file name to fusion url. Parameters: Name Type Description Default x str File path. required Returns (str): Fusion url string. Source code in fusion/utils.py def path_to_url ( x ): \"\"\"Convert file name to fusion url. Args: x (str): File path. Returns (str): Fusion url string. \"\"\" catalog , dataset , date , ext = _filename_to_distribution ( x . split ( \"/\" )[ - 1 ]) return \"/\" . join ( distribution_to_url ( \"\" , dataset , date , ext , catalog ) . split ( \"/\" )[ 1 :]) read_csv ( path , columns = None , filters = None , fs = None ) \u00b6 Reads csv with possibility of selecting columns and filtering the data. Parameters: Name Type Description Default path str path to the csv file. required columns list list of selected fields. None filters list filters. None fs filesystem object. None Returns: Type Description pandas.DataFrame a dataframe containing the data. Source code in fusion/utils.py def read_csv ( path : str , columns : list = None , filters : list = None , fs = None ): \"\"\"Reads csv with possibility of selecting columns and filtering the data. Args: path (str): path to the csv file. columns: list of selected fields. filters: filters. fs: filesystem object. Returns: pandas.DataFrame: a dataframe containing the data. \"\"\" try : try : tbl = _csv_to_table ( path , fs ) except Exception as err : logger . log ( VERBOSE_LVL , f 'Failed to read { path } , with comma delimiter. { err } ' , ) raise Exception out = BytesIO () pq . write_table ( tbl , out ) del tbl res = pq . read_table ( out , filters = filters , columns = columns ) . to_pandas () except Exception as err : logger . log ( VERBOSE_LVL , f \"Could not parse { path } properly. \" f \"Trying with pandas csv reader. { err } \" , ) try : with ( fs . open ( path ) if fs else nullcontext ( path )) as f : res = pd . read_csv ( f , usecols = columns , index_col = False ) except Exception as err : logger . log ( VERBOSE_LVL , f \"Could not parse { path } properly. \" f \"Trying with pandas csv reader pandas engine. { err } \" , ) with ( fs . open ( path ) if fs else nullcontext ( path )) as f : res = pd . read_table ( f , usecols = columns , index_col = False , engine = \"python\" , delimiter = None ) return res read_parquet ( path , columns = None , filters = None , fs = None ) \u00b6 Read parquet files(s) to pandas. Parameters: Name Type Description Default path Union[list, str] path or a list of paths to parquet files. required columns list list of selected fields. None filters list filters. None fs filesystem object. None Returns: Type Description pandas.DataFrame a dataframe containing the data. Source code in fusion/utils.py def read_parquet ( path : Union [ list , str ], columns : list = None , filters : list = None , fs = None ): \"\"\"Read parquet files(s) to pandas. Args: path (Union[list, str]): path or a list of paths to parquet files. columns (list): list of selected fields. filters (list): filters. fs: filesystem object. Returns: pandas.DataFrame: a dataframe containing the data. \"\"\" return pq . ParquetDataset ( path , use_legacy_dataset = False , filters = filters , filesystem = fs , memory_map = True ) . read_pandas ( columns = columns ) . to_pandas () stream_single_file_new_session ( credentials , url , output_file , overwrite = True , block_size = 65536 , dry_run = False , fs = None ) \u00b6 Function to stream a single file from the API to a file on disk. Parameters: Name Type Description Default credentials FusionCredentials Valid user credentials to provide an acces token required url str The URL to call. required output_file str The filename that the data will be saved into. required overwrite bool True if previously downloaded files should be overwritten. Defaults to True. True block_size int The chunk size to download data. Defaults to DEFAULT_CHUNK_SIZE 65536 dry_run bool Test that a file can be downloaded and return the filename without downloading the data. Defaults to False. False fs fsspec.filesystem Filesystem. None Returns: Type Description tuple Source code in fusion/utils.py def stream_single_file_new_session ( credentials , url : str , output_file : str , overwrite : bool = True , block_size = DEFAULT_CHUNK_SIZE , dry_run : bool = False , fs = None ) -> tuple : \"\"\"Function to stream a single file from the API to a file on disk. Args: credentials (FusionCredentials): Valid user credentials to provide an acces token url (str): The URL to call. output_file (str): The filename that the data will be saved into. overwrite (bool, optional): True if previously downloaded files should be overwritten. Defaults to True. block_size (int, optional): The chunk size to download data. Defaults to DEFAULT_CHUNK_SIZE dry_run (bool, optional): Test that a file can be downloaded and return the filename without downloading the data. Defaults to False. fs (fsspec.filesystem): Filesystem. Returns: tuple: A tuple \"\"\" if dry_run : return _stream_single_file_new_session_dry_run ( credentials , url , output_file ) output_file_path = Path ( output_file ) if not overwrite and output_file_path . exists (): return ( True , output_file , None ) tmp_name = output_file_path . with_name ( output_file_path . name + \".tmp\" ) try : with get_session ( credentials , url ) . get ( url , stream = True ) as r : r . raise_for_status () byte_cnt = 0 with fs . open ( tmp_name , \"wb\" ) as outfile : for chunk in r . iter_content ( block_size ): byte_cnt += len ( chunk ) outfile . write ( chunk ) tmp_name . rename ( output_file_path ) try : tmp_name . unlink () except FileNotFoundError : pass logger . log ( VERBOSE_LVL , f 'Wrote { byte_cnt : , } bytes to { output_file_path } , via { tmp_name } ' , ) return ( True , output_file , None ) except Exception as ex : logger . log ( VERBOSE_LVL , f 'Failed to write to { output_file_path } , via { tmp_name } . ex - { ex } ' , ) return False , output_file , ex upload_files ( fs_fusion , fs_local , loop , parallel = True , n_par =- 1 ) \u00b6 Upload file into Fusion. Parameters: Name Type Description Default fs_fusion Fusion filesystem. required fs_local Local filesystem. required loop iterable Loop of files to iterate through. required parallel bool Is parallel mode enabled. True n_par int Number of subprocesses. -1 Source code in fusion/utils.py def upload_files ( fs_fusion , fs_local , loop , parallel = True , n_par =- 1 ): \"\"\"Upload file into Fusion. Args: fs_fusion: Fusion filesystem. fs_local: Local filesystem. loop (iterable): Loop of files to iterate through. parallel (bool): Is parallel mode enabled. n_par (int): Number of subprocesses. Returns: List of update statuses. \"\"\" def _upload ( row ): kw = { \"headers\" : _construct_headers ( fs_local , row )} p_url = row [ \"url\" ] try : with fs_local . open ( row [ \"path\" ], \"rb\" ) as file_local : fs_fusion . put ( file_local , p_url , chunk_size = 100 * 2 ** 20 , method = \"put\" , ** kw ) return True , row [ \"path\" ], None except Exception as ex : logger . log ( VERBOSE_LVL , f 'Failed to upload { row [ \"path\" ] } . ex - { ex } ' , ) return False , row [ \"path\" ], ex if parallel : res = Parallel ( n_jobs = n_par )( delayed ( _upload )( row ) for index , row in loop ) else : res = [ _upload ( row ) for index , row in loop ] return res validate_file_names ( paths , fs_fusion ) \u00b6 Validate if the file name format adheres to the standard. Parameters: Name Type Description Default paths list List of file paths. required fs_fusion Fusion filesystem. required Returns (list): List of booleans. Source code in fusion/utils.py def validate_file_names ( paths , fs_fusion ): \"\"\"Validate if the file name format adheres to the standard. Args: paths (list): List of file paths. fs_fusion: Fusion filesystem. Returns (list): List of booleans. \"\"\" file_names = [ i . split ( \"/\" )[ - 1 ] . split ( \".\" )[ 0 ] for i in paths ] validation = [] all_catalogs = fs_fusion . ls ( '' ) all_datasets = {} for f_n in file_names : tmp = f_n . split ( '__' ) if len ( tmp ) == 3 : val = tmp [ 1 ] in all_catalogs if not val : validation . append ( False ) else : if tmp [ 1 ] not in all_datasets . keys (): all_datasets [ tmp [ 1 ]] = [ i . split ( '/' )[ - 1 ] for i in fs_fusion . ls ( f \" { tmp [ 1 ] } /datasets\" )] val = tmp [ 0 ] in all_datasets [ tmp [ 1 ]] validation . append ( val ) else : validation . append ( False ) if not all ( validation ): for i , p in enumerate ( paths ): if not validation [ i ]: logger . warning ( f \"The file in { p } has a non-compliant name and will not be processed. \" f \"Please rename the file to dataset__catalog__yyyymmdd.format\" ) return validation","title":"Modules"},{"location":"api/#fusion.authentication","text":"Fusion authentication module.","title":"authentication"},{"location":"api/#fusion.authentication.FusionCredentials","text":"Utility functions to manage credentials.","title":"FusionCredentials"},{"location":"api/#fusion.authentication.FusionCredentials.__init__","text":"Constructor for the FusionCredentials authentication management class. Parameters: Name Type Description Default client_id str A valid OAuth client identifier. Defaults to None. None client_secret str A valid OAuth client secret. Defaults to None. None username str A valid username. Defaults to None. None password str A valid password for the username. Defaults to None. None resource str The OAuth audience. Defaults to None. None auth_url str URL for the OAuth authentication server. Defaults to None. None proxies dict Any proxy servers required to route HTTP and HTTPS requests to the internet. {} grant_type str Allows the grant type to be changed to support different credential types. Defaults to client_credentials. 'client_credentials' Source code in fusion/authentication.py def __init__ ( self , client_id : str = None , client_secret : str = None , username : str = None , password : str = None , resource : str = None , auth_url : str = None , proxies = {}, grant_type : str = 'client_credentials' , ) -> None : \"\"\"Constructor for the FusionCredentials authentication management class. Args: client_id (str, optional): A valid OAuth client identifier. Defaults to None. client_secret (str, optional): A valid OAuth client secret. Defaults to None. username (str, optional): A valid username. Defaults to None. password (str, optional): A valid password for the username. Defaults to None. resource (str, optional): The OAuth audience. Defaults to None. auth_url (str, optional): URL for the OAuth authentication server. Defaults to None. proxies (dict, optional): Any proxy servers required to route HTTP and HTTPS requests to the internet. grant_type (str, optional): Allows the grant type to be changed to support different credential types. Defaults to client_credentials. \"\"\" self . client_id = client_id self . client_secret = client_secret self . username = username self . password = password self . resource = resource self . auth_url = auth_url self . proxies = proxies self . grant_type = grant_type","title":"__init__()"},{"location":"api/#fusion.authentication.FusionCredentials.add_proxies","text":"A function to add proxies to an existing credentials files. This function can be called to add proxy addresses to a credential file downloaded from the Fusion website. Parameters: Name Type Description Default http_proxy str The HTTP proxy address. required https_proxy str The HTTPS proxy address. If not specified then this will be the copied form the HTTP proxy. None credentials_file str The path and filename to store the credentials under. Path may be absolute or relative to current working directory. Defaults to 'config/client_credentials.json'. 'config/client_credentials.json' Returns: Type Description None None Source code in fusion/authentication.py @staticmethod def add_proxies ( http_proxy : str , https_proxy : str = None , credentials_file : str = 'config/client_credentials.json' ) -> None : \"\"\"A function to add proxies to an existing credentials files. This function can be called to add proxy addresses to a credential file downloaded from the Fusion website. Args: http_proxy (str): The HTTP proxy address. https_proxy (str): The HTTPS proxy address. If not specified then this will be the copied form the HTTP proxy. credentials_file (str, optional): The path and filename to store the credentials under. Path may be absolute or relative to current working directory. Defaults to 'config/client_credentials.json'. Returns: None \"\"\" credentials = FusionCredentials . from_file ( credentials_file ) credentials . proxies [ 'http' ] = http_proxy if https_proxy is None : https_proxy = http_proxy credentials . proxies [ 'https' ] = https_proxy data : Dict [ str , Union [ str , dict ]] = dict ( { 'client_id' : credentials . client_id , 'client_secret' : credentials . client_secret , 'resource' : credentials . resource , 'auth_url' : credentials . auth_url , 'proxies' : credentials . proxies , } ) json_data = json . dumps ( data , indent = 4 ) with open ( credentials_file , 'w' ) as credentialsfile : credentialsfile . write ( json_data ) return","title":"add_proxies()"},{"location":"api/#fusion.authentication.FusionCredentials.from_dict","text":"Create a credentials object from a dictionary. This is the only FusionCredentials creation method that supports the password grant type since the username and password should be provided by the user. Parameters: Name Type Description Default credentials dict A dictionary containing the requried keys: client_id, client_secret, resource, auth_url, and optionally proxies and an OAuth grant type. required Returns: Type Description FusionCredentials a credentials object that can be used for authentication. Source code in fusion/authentication.py @staticmethod def from_dict ( credentials : dict ): \"\"\"Create a credentials object from a dictionary. This is the only FusionCredentials creation method that supports the password grant type since the username and password should be provided by the user. Args: credentials (dict): A dictionary containing the requried keys: client_id, client_secret, resource, auth_url, and optionally proxies and an OAuth grant type. Returns: FusionCredentials: a credentials object that can be used for authentication. \"\"\" if 'grant_type' in credentials : grant_type = credentials [ 'grant_type' ] else : grant_type = 'client_credentials' client_id = credentials [ 'client_id' ] if grant_type == 'client_credentials' : client_secret = credentials [ 'client_secret' ] username = None password = None elif grant_type == 'password' : client_secret = None username = credentials [ 'username' ] password = credentials [ 'password' ] else : raise CredentialError ( f 'Unrecognised grant type { grant_type } ' ) resource = credentials [ 'resource' ] auth_url = credentials [ 'auth_url' ] proxies = credentials . get ( 'proxies' ) creds = FusionCredentials ( client_id , client_secret , username , password , resource , auth_url , proxies , grant_type = grant_type ) return creds","title":"from_dict()"},{"location":"api/#fusion.authentication.FusionCredentials.from_file","text":"Create a credentials object from a file. Parameters: Name Type Description Default file_path str Path (absolute or relative) and filename to load credentials from. Defaults to 'config/client.credentials.json'. 'config/client.credentials.json' fs fsspec.filesystem Filesystem to use. None walk_up_dirs bool if true it walks up the directories in search of a config folder True Returns: Type Description FusionCredentials a credentials object that can be used for authentication. Source code in fusion/authentication.py @staticmethod def from_file ( file_path : str = 'config/client.credentials.json' , fs = None , walk_up_dirs = True ): \"\"\"Create a credentials object from a file. Args: file_path (str, optional): Path (absolute or relative) and filename to load credentials from. Defaults to 'config/client.credentials.json'. fs (fsspec.filesystem): Filesystem to use. walk_up_dirs (bool): if true it walks up the directories in search of a config folder Returns: FusionCredentials: a credentials object that can be used for authentication. \"\"\" fs = fs if fs else get_default_fs () to_use_file_path = None if fs . exists ( file_path ): # absolute path case logger . log ( VERBOSE_LVL , f \"Found credentials file at { file_path } \" ) to_use_file_path = file_path elif fs . exists ( os . path . join ( fs . info ( \"\" )[ \"name\" ], file_path )): # relative path case to_use_file_path = os . path . join ( fs . info ( \"\" )[ \"name\" ], file_path ) logger . log ( VERBOSE_LVL , f \"Found credentials file at { to_use_file_path } \" ) else : for p in [ s . __str__ () for s in Path ( fs . info ( \"\" )[ \"name\" ]) . parents ]: if fs . exists ( os . path . join ( p , file_path )): to_use_file_path = os . path . join ( p , file_path ) logger . log ( VERBOSE_LVL , f \"Found credentials file at { to_use_file_path } \" ) break if fs . size ( to_use_file_path ) > 0 : try : with fs . open ( to_use_file_path , 'r' ) as credentials : data = json . load ( credentials ) credentials = FusionCredentials . from_dict ( data ) return credentials except Exception as e : print ( e ) logger . error ( e ) raise Exception ( e ) else : msg = f \" { to_use_file_path } is an empty file, make sure to add your credentials to it.\" print ( msg ) logger . error ( msg ) raise IOError ( msg )","title":"from_file()"},{"location":"api/#fusion.authentication.FusionCredentials.from_object","text":"Utility function that will determine how to create a credentials object based on data passed. Parameters: Name Type Description Default credentials_source Union[str, dict] A string which could be a filename or a JSON object, or a dictionary. required Exceptions: Type Description CredentialError Exception raised when the provided credentials is not one of the supported types Returns: Type Description FusionCredentials a credentials object that can be used for authentication. Source code in fusion/authentication.py @staticmethod def from_object ( credentials_source : Union [ str , dict ]): \"\"\"Utility function that will determine how to create a credentials object based on data passed. Args: credentials_source (Union[str, dict]): A string which could be a filename or a JSON object, or a dictionary. Raises: CredentialError: Exception raised when the provided credentials is not one of the supported types Returns: FusionCredentials: a credentials object that can be used for authentication. \"\"\" if isinstance ( credentials_source , dict ): return FusionCredentials . from_dict ( credentials_source ) elif isinstance ( credentials_source , str ): if _is_json ( credentials_source ): return FusionCredentials . from_dict ( json . loads ( credentials_source )) else : return FusionCredentials . from_file ( credentials_source ) raise CredentialError ( f 'Could not resolve the credentials provided: { credentials_source } ' )","title":"from_object()"},{"location":"api/#fusion.authentication.FusionCredentials.generate_credentials_file","text":"Utility function to generate credentials file that can be used for authentication. Parameters: Name Type Description Default credentials_file str The path and filename to store the credentials under. Path may be absolute or relative to current working directory. Defaults to 'config/client_credentials.json'. 'config/client_credentials.json' client_id str A valid OAuth client identifier. Defaults to None. None client_secret str A valid OAuth client secret. Defaults to None. None resource str The OAuth audience. Defaults to None. 'JPMC:URI:RS-93742-Fusion-PROD' auth_url str URL for the OAuth authentication server. Defaults to None. 'https://authe.jpmorgan.com/as/token.oauth2' proxies Union[str, dict] Any proxy servers required to route HTTP and HTTPS requests to the internet. Defaults to {}. Keys are http and https. Or specify a single URL to set both http and https None Exceptions: Type Description CredentialError Exception to handle missing values required for authentication. Returns: Type Description FusionCredentials a credentials object that can be used for authentication. Source code in fusion/authentication.py @staticmethod def generate_credentials_file ( credentials_file : str = 'config/client_credentials.json' , client_id : str = None , client_secret : str = None , resource : str = \"JPMC:URI:RS-93742-Fusion-PROD\" , auth_url : str = \"https://authe.jpmorgan.com/as/token.oauth2\" , proxies : Union [ str , dict ] = None , ): \"\"\"Utility function to generate credentials file that can be used for authentication. Args: credentials_file (str, optional): The path and filename to store the credentials under. Path may be absolute or relative to current working directory. Defaults to 'config/client_credentials.json'. client_id (str, optional): A valid OAuth client identifier. Defaults to None. client_secret (str, optional): A valid OAuth client secret. Defaults to None. resource (str, optional): The OAuth audience. Defaults to None. auth_url (str, optional): URL for the OAuth authentication server. Defaults to None. proxies (Union[str, dict], optional): Any proxy servers required to route HTTP and HTTPS requests to the internet. Defaults to {}. Keys are http and https. Or specify a single URL to set both http and https Raises: CredentialError: Exception to handle missing values required for authentication. Returns: FusionCredentials: a credentials object that can be used for authentication. \"\"\" if not client_id : raise CredentialError ( 'A valid client_id is required' ) if not client_secret : raise CredentialError ( 'A valid client secret is required' ) data : Dict [ str , Union [ str , dict ]] = dict ( { 'client_id' : client_id , 'client_secret' : client_secret , 'resource' : resource , 'auth_url' : auth_url } ) proxies_resolved = {} if proxies : if isinstance ( proxies , dict ): raw_proxies_dict = proxies elif isinstance ( proxies , str ): if _is_url ( proxies ): raw_proxies_dict = { 'http' : proxies , 'https' : proxies } elif _is_json ( proxies ): raw_proxies_dict = json . loads ( proxies ) else : raise CredentialError ( f 'A valid proxies param is required, [ { proxies } ] is not supported.' ) # Now validate and conform proxies dict valid_pxy_keys = [ 'http' , 'https' , 'http_proxy' , 'https_proxy' ] pxy_key_map = { 'http' : 'http' , 'https' : 'https' , 'http_proxy' : 'http' , 'https_proxy' : 'https' , } lcase_dict = { k . lower (): v for k , v in raw_proxies_dict . items ()} if set ( lcase_dict . keys ()) . intersection ( set ( valid_pxy_keys )) != set ( lcase_dict . keys ()): raise CredentialError ( f 'Invalid proxies keys in dict { raw_proxies_dict . keys () } .' f 'Only { pxy_key_map . keys () } are accepted and will be mapped as necessary.' ) proxies_resolved = { pxy_key_map [ k ]: v for k , v in lcase_dict . items ()} data [ 'proxies' ] = proxies_resolved json_data = json . dumps ( data , indent = 4 ) Path ( credentials_file ) . parent . mkdir ( parents = True , exist_ok = True ) with open ( credentials_file , 'w' ) as credentialsfile : credentialsfile . write ( json_data ) credentials = FusionCredentials . from_file ( file_path = credentials_file ) return credentials","title":"generate_credentials_file()"},{"location":"api/#fusion.authentication.FusionOAuthAdapter","text":"An OAuth adapter to manage authentication and session tokens.","title":"FusionOAuthAdapter"},{"location":"api/#fusion.authentication.FusionOAuthAdapter.__init__","text":"Class constructor to create a FusionOAuthAdapter object. Parameters: Name Type Description Default credentials Union[fusion.authentication.FusionCredentials, str, dict] Valid user credentials to authenticate. required proxies dict Specify a proxy if required to access the authentication server. Defaults to {}. {} refresh_within_seconds int When an API call is made with less than the specified number of seconds until the access token expires, or after expiry, it will refresh the token. Defaults to 5. 5 auth_retries Union[int, urllib3.util.retry.Retry] Number of times to attempt to authenticate to handle connection problems. Defaults to None. None Source code in fusion/authentication.py def __init__ ( self , credentials : Union [ FusionCredentials , Union [ str , dict ]], proxies : dict = {}, refresh_within_seconds : int = 5 , auth_retries : Union [ int , Retry ] = None , * args , ** kwargs , ) -> None : \"\"\"Class constructor to create a FusionOAuthAdapter object. Args: credentials (Union[FusionCredentials, Union[str, dict]): Valid user credentials to authenticate. proxies (dict, optional): Specify a proxy if required to access the authentication server. Defaults to {}. refresh_within_seconds (int, optional): When an API call is made with less than the specified number of seconds until the access token expires, or after expiry, it will refresh the token. Defaults to 5. auth_retries (Union[int, Retry]): Number of times to attempt to authenticate to handle connection problems. Defaults to None. \"\"\" super ( FusionOAuthAdapter , self ) . __init__ ( * args , ** kwargs ) if isinstance ( credentials , FusionCredentials ): self . credentials = credentials else : self . credentials = FusionCredentials . from_object ( credentials ) if proxies : self . proxies = proxies else : self . proxies = self . credentials . proxies self . bearer_token_expiry = datetime . datetime . now () self . number_token_refreshes = 0 self . refresh_within_seconds = refresh_within_seconds if not auth_retries : self . auth_retries = Retry ( total = 20 , backoff_factor = 0.2 ) else : self . auth_retries = Retry . from_int ( auth_retries )","title":"__init__()"},{"location":"api/#fusion.authentication.FusionOAuthAdapter.send","text":"Function to send a request to the authentication server. Parameters: Name Type Description Default request requests.Session A HTTP Session. required Source code in fusion/authentication.py def send ( self , request , ** kwargs ): \"\"\"Function to send a request to the authentication server. Args: request (requests.Session): A HTTP Session. Returns: \"\"\" def _refresh_token_data (): payload = ( { \"grant_type\" : \"client_credentials\" , \"client_id\" : self . credentials . client_id , \"client_secret\" : self . credentials . client_secret , \"aud\" : self . credentials . resource , } if self . credentials . grant_type == 'client_credentials' else { \"grant_type\" : \"password\" , \"client_id\" : self . credentials . client_id , \"username\" : self . credentials . username , \"password\" : self . credentials . password , \"resource\" : self . credentials . resource , } ) try : s = requests . Session () if self . proxies : # mypy does note recognise session.proxies as a dict so fails this line, we'll ignore this chk s . proxies . update ( self . proxies ) # type:ignore s . mount ( 'http://' , HTTPAdapter ( max_retries = self . auth_retries )) response = s . post ( self . credentials . auth_url , data = payload ) response_data = response . json () access_token = response_data [ \"access_token\" ] expiry = response_data [ \"expires_in\" ] return access_token , expiry except Exception as ex : raise Exception ( f 'Failed to authenticate against OAuth server { ex } ' ) token_expires_in = ( self . bearer_token_expiry - datetime . datetime . now ()) . total_seconds () if token_expires_in < self . refresh_within_seconds : token , expiry = _refresh_token_data () self . token = token self . bearer_token_expiry = datetime . datetime . now () + timedelta ( seconds = int ( expiry )) self . number_token_refreshes += 1 logger . log ( VERBOSE_LVL , f 'Refreshed token { self . number_token_refreshes } time { _res_plural ( self . number_token_refreshes ) } ' , ) request . headers . update ({ 'Authorization' : f 'Bearer { self . token } ' }) response = super ( FusionOAuthAdapter , self ) . send ( request , ** kwargs ) return response","title":"send()"},{"location":"api/#fusion.authentication.get_default_fs","text":"Retrieve default filesystem. Returns: filesystem Source code in fusion/authentication.py def get_default_fs (): \"\"\"Retrieve default filesystem. Returns: filesystem \"\"\" protocol = \"file\" if \"FS_PROTOCOL\" not in os . environ . keys () else os . environ [ \"FS_PROTOCOL\" ] if ( \"S3_ENDPOINT\" in os . environ . keys () and \"AWS_ACCESS_KEY_ID\" in os . environ . keys () and \"AWS_SECRET_ACCESS_KEY\" in os . environ . keys () ): endpoint = os . environ [ \"S3_ENDPOINT\" ] fs = fsspec . filesystem ( \"s3\" , client_kwargs = { \"endpoint_url\" : f \"https:// { endpoint } \" }, key = os . environ [ \"AWS_ACCESS_KEY_ID\" ], secret = os . environ [ \"AWS_SECRET_ACCESS_KEY\" ], ) else : fs = fsspec . filesystem ( protocol ) return fs","title":"get_default_fs()"},{"location":"api/#fusion.exceptions","text":"Bespoke exceptions and errors.","title":"exceptions"},{"location":"api/#fusion.exceptions.APIConnectError","text":"APIConnectError exception wrapper to handle API connection errors. Parameters: Name Type Description Default Exception Exception to wrap. required","title":"APIConnectError"},{"location":"api/#fusion.exceptions.APIRequestError","text":"APIRequestError exception wrapper to handle API request erorrs. Parameters: Name Type Description Default Exception Exception to wrap. required","title":"APIRequestError"},{"location":"api/#fusion.exceptions.APIResponseError","text":"APIResponseError exception wrapper to handle API response errors. Parameters: Name Type Description Default Exception Exception to wrap. required","title":"APIResponseError"},{"location":"api/#fusion.exceptions.CredentialError","text":"CredentialError exception wrapper to handle errors in credentials provided for authentication. Parameters: Name Type Description Default Exception Exception to wrap. required","title":"CredentialError"},{"location":"api/#fusion.exceptions.UnrecognizedFormatError","text":"UnrecognizedFormatError exception wrapper to handle format errors. Parameters: Name Type Description Default Exception Exception to wrap. required","title":"UnrecognizedFormatError"},{"location":"api/#fusion.fusion","text":"Main Fusion module.","title":"fusion"},{"location":"api/#fusion.fusion.Fusion","text":"Core Fusion class for API access.","title":"Fusion"},{"location":"api/#fusion.fusion.Fusion.default_catalog","text":"Returns the default catalog. Returns: Type Description str None","title":"default_catalog"},{"location":"api/#fusion.fusion.Fusion.__init__","text":"Constructor to instantiate a new Fusion object. Parameters: Name Type Description Default credentials Union[str, dict] A path to a credentials file or a dictionary containing the required keys. Defaults to 'config/client_credentials.json'. 'config/client_credentials.json' root_url str The API root URL. Defaults to \"https://fusion-api.jpmorgan.com/fusion/v1/\". 'https://fusion-api.jpmorgan.com/fusion/v1/' download_folder str The folder path where downloaded data files are saved. Defaults to \"downloads\". 'downloads' log_level int Set the logging level. Defaults to logging.ERROR. 40 fs fsspec.filesystem filesystem. None Source code in fusion/fusion.py def __init__ ( self , credentials : Union [ str , dict ] = 'config/client_credentials.json' , root_url : str = \"https://fusion-api.jpmorgan.com/fusion/v1/\" , download_folder : str = \"downloads\" , log_level : int = logging . ERROR , fs = None , ) -> None : \"\"\"Constructor to instantiate a new Fusion object. Args: credentials (Union[str, dict], optional): A path to a credentials file or a dictionary containing the required keys. Defaults to 'config/client_credentials.json'. root_url (_type_, optional): The API root URL. Defaults to \"https://fusion-api.jpmorgan.com/fusion/v1/\". download_folder (str, optional): The folder path where downloaded data files are saved. Defaults to \"downloads\". log_level (int, optional): Set the logging level. Defaults to logging.ERROR. fs (fsspec.filesystem): filesystem. \"\"\" self . _default_catalog = \"common\" self . root_url = root_url self . download_folder = download_folder Path ( download_folder ) . mkdir ( parents = True , exist_ok = True ) if logger . hasHandlers (): logger . handlers . clear () file_handler = logging . FileHandler ( filename = \"fusion_sdk.log\" ) logging . addLevelName ( VERBOSE_LVL , \"VERBOSE\" ) stdout_handler = logging . StreamHandler ( sys . stdout ) formatter = logging . Formatter ( \" %(asctime)s . %(msecs)03d %(name)s : %(levelname)s %(message)s \" , datefmt = \"%Y-%m- %d %H:%M:%S\" ) stdout_handler . setFormatter ( formatter ) logger . addHandler ( stdout_handler ) logger . addHandler ( file_handler ) logger . setLevel ( log_level ) if isinstance ( credentials , FusionCredentials ): self . credentials = credentials else : self . credentials = FusionCredentials . from_object ( credentials ) self . session = get_session ( self . credentials , self . root_url ) self . fs = fs if fs else get_default_fs ()","title":"__init__()"},{"location":"api/#fusion.fusion.Fusion.__repr__","text":"Object representation to list all available methods. Source code in fusion/fusion.py def __repr__ ( self ): \"\"\"Object representation to list all available methods.\"\"\" return \"Fusion object \\n Available methods: \\n \" + tabulate ( pd . DataFrame ( [ [ method_name for method_name in dir ( Fusion ) if callable ( getattr ( Fusion , method_name )) and not method_name . startswith ( \"_\" ) ] + [ p for p in dir ( Fusion ) if isinstance ( getattr ( Fusion , p ), property )] ] ) . T . set_index ( 0 ), tablefmt = \"psql\" , )","title":"__repr__()"},{"location":"api/#fusion.fusion.Fusion.catalog_resources","text":"List the resources contained within the catalog, for example products and datasets. Parameters: Name Type Description Default catalog str A catalog identifier. Defaults to 'common'. None output bool If True then print the dataframe. Defaults to False. False Returns: Type Description DataFrame pandas.DataFrame: A dataframe with a row for each resource within the catalog Source code in fusion/fusion.py def catalog_resources ( self , catalog : str = None , output : bool = False ) -> pd . DataFrame : \"\"\"List the resources contained within the catalog, for example products and datasets. Args: catalog (str, optional): A catalog identifier. Defaults to 'common'. output (bool, optional): If True then print the dataframe. Defaults to False. Returns: pandas.DataFrame: A dataframe with a row for each resource within the catalog \"\"\" catalog = self . __use_catalog ( catalog ) url = f ' { self . root_url } catalogs/ { catalog } ' df = Fusion . _call_for_dataframe ( url , self . session ) if output : print ( tabulate ( df , headers = \"keys\" , tablefmt = \"psql\" , maxcolwidths = 30 )) return df","title":"catalog_resources()"},{"location":"api/#fusion.fusion.Fusion.dataset_resources","text":"List the resources available for a dataset, currently this will always be a datasetseries. Parameters: Name Type Description Default dataset str A dataset identifier required catalog str A catalog identifier. Defaults to 'common'. None output bool If True then print the dataframe. Defaults to False. False Returns: Type Description DataFrame pandas.DataFrame: A dataframe with a row for each resource Source code in fusion/fusion.py def dataset_resources ( self , dataset : str , catalog : str = None , output : bool = False ) -> pd . DataFrame : \"\"\"List the resources available for a dataset, currently this will always be a datasetseries. Args: dataset (str): A dataset identifier catalog (str, optional): A catalog identifier. Defaults to 'common'. output (bool, optional): If True then print the dataframe. Defaults to False. Returns: pandas.DataFrame: A dataframe with a row for each resource \"\"\" catalog = self . __use_catalog ( catalog ) url = f ' { self . root_url } catalogs/ { catalog } /datasets/ { dataset } ' df = Fusion . _call_for_dataframe ( url , self . session ) if output : print ( tabulate ( df , headers = \"keys\" , tablefmt = \"psql\" )) return df","title":"dataset_resources()"},{"location":"api/#fusion.fusion.Fusion.datasetmember_resources","text":"List the available resources for a datasetseries member. Parameters: Name Type Description Default dataset str A dataset identifier required series str The datasetseries identifier required catalog str A catalog identifier. Defaults to 'common'. None output bool If True then print the dataframe. Defaults to False. False Returns: Type Description DataFrame pandas.DataFrame: A dataframe with a row for each datasetseries member resource. Currently, this will always be distributions. Source code in fusion/fusion.py def datasetmember_resources ( self , dataset : str , series : str , catalog : str = None , output : bool = False ) -> pd . DataFrame : \"\"\"List the available resources for a datasetseries member. Args: dataset (str): A dataset identifier series (str): The datasetseries identifier catalog (str, optional): A catalog identifier. Defaults to 'common'. output (bool, optional): If True then print the dataframe. Defaults to False. Returns: pandas.DataFrame: A dataframe with a row for each datasetseries member resource. Currently, this will always be distributions. \"\"\" catalog = self . __use_catalog ( catalog ) url = f ' { self . root_url } catalogs/ { catalog } /datasets/ { dataset } /datasetseries/ { series } ' df = Fusion . _call_for_dataframe ( url , self . session ) if output : print ( tabulate ( df , headers = \"keys\" , tablefmt = \"psql\" )) return df","title":"datasetmember_resources()"},{"location":"api/#fusion.fusion.Fusion.download","text":"Downloads the requested distributions of a dataset to disk. Parameters: Name Type Description Default dataset str A dataset identifier required dt_str str Either a single date or a range identified by a start or end date, or both separated with a \":\". Defaults to 'latest' which will return the most recent instance of the dataset. 'latest' dataset_format str The file format, e.g. CSV or Parquet. Defaults to 'parquet'. 'parquet' catalog str A catalog identifier. Defaults to 'common'. None n_par int Specify how many distributions to download in parallel. Defaults to all cpus available. None show_progress bool Display a progress bar during data download Defaults to True. True force_download bool If True then will always download a file even if it is already on disk. Defaults to True. False download_folder str The path, absolute or relative, where downloaded files are saved. Defaults to download_folder as set in init None return_paths bool Return paths and success statuses of the downloaded files. False Source code in fusion/fusion.py def download ( self , dataset : str , dt_str : str = 'latest' , dataset_format : str = 'parquet' , catalog : str = None , n_par : int = None , show_progress : bool = True , force_download : bool = False , download_folder : str = None , return_paths : bool = False , ): \"\"\"Downloads the requested distributions of a dataset to disk. Args: dataset (str): A dataset identifier dt_str (str, optional): Either a single date or a range identified by a start or end date, or both separated with a \":\". Defaults to 'latest' which will return the most recent instance of the dataset. dataset_format (str, optional): The file format, e.g. CSV or Parquet. Defaults to 'parquet'. catalog (str, optional): A catalog identifier. Defaults to 'common'. n_par (int, optional): Specify how many distributions to download in parallel. Defaults to all cpus available. show_progress (bool, optional): Display a progress bar during data download Defaults to True. force_download (bool, optional): If True then will always download a file even if it is already on disk. Defaults to True. download_folder (str, optional): The path, absolute or relative, where downloaded files are saved. Defaults to download_folder as set in __init__ return_paths (bool, optional): Return paths and success statuses of the downloaded files. Returns: \"\"\" catalog = self . __use_catalog ( catalog ) n_par = cpu_count ( n_par ) required_series = self . _resolve_distro_tuples ( dataset , dt_str , dataset_format , catalog ) if not download_folder : download_folder = self . download_folder if not self . fs . exists ( download_folder ): self . fs . mkdir ( download_folder , create_parents = True ) download_spec = [ { \"credentials\" : self . credentials , \"url\" : distribution_to_url ( self . root_url , series [ 1 ], series [ 2 ], series [ 3 ], series [ 0 ]), \"output_file\" : distribution_to_filename ( download_folder , series [ 1 ], series [ 2 ], series [ 3 ], series [ 0 ]), \"overwrite\" : force_download , \"fs\" : self . fs } for series in required_series ] if show_progress : loop = tqdm ( download_spec ) else : loop = download_spec logger . log ( VERBOSE_LVL , f 'Beginning { len ( loop ) } downloads in batches of { n_par } ' , ) res = Parallel ( n_jobs = n_par )( delayed ( stream_single_file_new_session )( ** spec ) for spec in loop ) return res if return_paths else None","title":"download()"},{"location":"api/#fusion.fusion.Fusion.get_fusion_filesystem","text":"Creates Fusion Filesystem. Returns: Fusion Filesystem Source code in fusion/fusion.py def get_fusion_filesystem ( self ): \"\"\"Creates Fusion Filesystem. Returns: Fusion Filesystem \"\"\" return FusionHTTPFileSystem ( client_kwargs = { \"root_url\" : self . root_url , \"credentials\" : self . credentials })","title":"get_fusion_filesystem()"},{"location":"api/#fusion.fusion.Fusion.list_catalogs","text":"Lists the catalogs available to the API account. Parameters: Name Type Description Default output bool If True then print the dataframe. Defaults to False. False Returns: Type Description DataFrame pandas.DataFrame: A dataframe with a row for each catalog Source code in fusion/fusion.py def list_catalogs ( self , output : bool = False ) -> pd . DataFrame : \"\"\"Lists the catalogs available to the API account. Args: output (bool, optional): If True then print the dataframe. Defaults to False. Returns: pandas.DataFrame: A dataframe with a row for each catalog \"\"\" url = f ' { self . root_url } catalogs/' df = Fusion . _call_for_dataframe ( url , self . session ) if output : print ( tabulate ( df , headers = \"keys\" , tablefmt = \"psql\" )) return df","title":"list_catalogs()"},{"location":"api/#fusion.fusion.Fusion.list_dataset_attributes","text":"Returns the list of attributes that are in the dataset. Parameters: Name Type Description Default dataset str A dataset identifier required catalog str A catalog identifier. Defaults to 'common'. None output bool If True then print the dataframe. Defaults to False. False display_all_columns bool If True displays all columns returned by the API, otherwise only the key columns are displayed False Returns: Type Description DataFrame pandas.DataFrame: A dataframe with a row for each attribute Source code in fusion/fusion.py def list_dataset_attributes ( self , dataset : str , catalog : str = None , output : bool = False , display_all_columns : bool = False ) -> pd . DataFrame : \"\"\"Returns the list of attributes that are in the dataset. Args: dataset (str): A dataset identifier catalog (str, optional): A catalog identifier. Defaults to 'common'. output (bool, optional): If True then print the dataframe. Defaults to False. display_all_columns (bool, optional): If True displays all columns returned by the API, otherwise only the key columns are displayed Returns: pandas.DataFrame: A dataframe with a row for each attribute \"\"\" catalog = self . __use_catalog ( catalog ) url = f ' { self . root_url } catalogs/ { catalog } /datasets/ { dataset } /attributes' df = Fusion . _call_for_dataframe ( url , self . session ) if not display_all_columns : df = df [[ \"identifier\" , \"dataType\" , \"isDatasetKey\" , \"description\" ]] if output : print ( tabulate ( df , headers = \"keys\" , tablefmt = \"psql\" , maxcolwidths = 30 )) return df","title":"list_dataset_attributes()"},{"location":"api/#fusion.fusion.Fusion.list_datasetmembers","text":"List the available members in the dataset series. Parameters: Name Type Description Default dataset str A dataset identifier required catalog str A catalog identifier. Defaults to 'common'. None output bool If True then print the dataframe. Defaults to False. False max_results int Limit the number of rows returned in the dataframe. Defaults to -1 which returns all results. -1 Returns: Type Description DataFrame pandas.DataFrame: a dataframe with a row for each dataset member. Source code in fusion/fusion.py def list_datasetmembers ( self , dataset : str , catalog : str = None , output : bool = False , max_results : int = - 1 ) -> pd . DataFrame : \"\"\"List the available members in the dataset series. Args: dataset (str): A dataset identifier catalog (str, optional): A catalog identifier. Defaults to 'common'. output (bool, optional): If True then print the dataframe. Defaults to False. max_results (int, optional): Limit the number of rows returned in the dataframe. Defaults to -1 which returns all results. Returns: pandas.DataFrame: a dataframe with a row for each dataset member. \"\"\" catalog = self . __use_catalog ( catalog ) url = f ' { self . root_url } catalogs/ { catalog } /datasets/ { dataset } /datasetseries' df = Fusion . _call_for_dataframe ( url , self . session ) if max_results > - 1 : df = df [ 0 : max_results ] if output : print ( tabulate ( df , headers = \"keys\" , tablefmt = \"psql\" )) return df","title":"list_datasetmembers()"},{"location":"api/#fusion.fusion.Fusion.list_datasets","text":"summary . Parameters: Name Type Description Default contains Union[str, list] A string or a list of strings that are dataset identifiers to filter the datasets list. If a list is provided then it will return datasets whose identifier matches any of the strings. Defaults to None. None id_contains bool Filter datasets only where the string(s) are contained in the identifier, ignoring description. False catalog str A catalog identifier. Defaults to 'common'. None output bool If True then print the dataframe. Defaults to False. False max_results int Limit the number of rows returned in the dataframe. Defaults to -1 which returns all results. -1 display_all_columns bool If True displays all columns returned by the API, otherwise only the key columns are displayed False Returns: Type Description DataFrame pandas.DataFrame: a dataframe with a row for each dataset. Source code in fusion/fusion.py def list_datasets ( self , contains : Union [ str , list ] = None , id_contains : bool = False , catalog : str = None , output : bool = False , max_results : int = - 1 , display_all_columns : bool = False , ) -> pd . DataFrame : \"\"\"_summary_. Args: contains (Union[str, list], optional): A string or a list of strings that are dataset identifiers to filter the datasets list. If a list is provided then it will return datasets whose identifier matches any of the strings. Defaults to None. id_contains (bool): Filter datasets only where the string(s) are contained in the identifier, ignoring description. catalog (str, optional): A catalog identifier. Defaults to 'common'. output (bool, optional): If True then print the dataframe. Defaults to False. max_results (int, optional): Limit the number of rows returned in the dataframe. Defaults to -1 which returns all results. display_all_columns (bool, optional): If True displays all columns returned by the API, otherwise only the key columns are displayed Returns: pandas.DataFrame: a dataframe with a row for each dataset. \"\"\" catalog = self . __use_catalog ( catalog ) url = f ' { self . root_url } catalogs/ { catalog } /datasets' df = Fusion . _call_for_dataframe ( url , self . session ) if contains : if isinstance ( contains , list ): contains = \"|\" . join ( f ' { s } ' for s in contains ) if id_contains : df = df [ df [ 'identifier' ] . str . contains ( contains , case = False )] else : df = df [ df [ 'identifier' ] . str . contains ( contains , case = False ) | df [ 'description' ] . str . contains ( contains , case = False ) ] if max_results > - 1 : df = df [ 0 : max_results ] df [ \"category\" ] = df . category . str . join ( \", \" ) df [ \"region\" ] = df . region . str . join ( \", \" ) if not display_all_columns : df = df [ [ \"identifier\" , \"title\" , \"region\" , \"category\" , \"coverageStartDate\" , \"coverageEndDate\" , \"description\" ] ] if output : print ( tabulate ( df , headers = \"keys\" , tablefmt = \"psql\" , maxcolwidths = 30 )) return df","title":"list_datasets()"},{"location":"api/#fusion.fusion.Fusion.list_distributions","text":"List the available distributions (downloadable instances of the dataset with a format type). Parameters: Name Type Description Default dataset str A dataset identifier required series str The datasetseries identifier required catalog str A catalog identifier. Defaults to 'common'. None output bool If True then print the dataframe. Defaults to False. False Returns: Type Description DataFrame pandas.DataFrame: A dataframe with a row for each distribution. Source code in fusion/fusion.py def list_distributions ( self , dataset : str , series : str , catalog : str = None , output : bool = False ) -> pd . DataFrame : \"\"\"List the available distributions (downloadable instances of the dataset with a format type). Args: dataset (str): A dataset identifier series (str): The datasetseries identifier catalog (str, optional): A catalog identifier. Defaults to 'common'. output (bool, optional): If True then print the dataframe. Defaults to False. Returns: pandas.DataFrame: A dataframe with a row for each distribution. \"\"\" catalog = self . __use_catalog ( catalog ) url = f ' { self . root_url } catalogs/ { catalog } /datasets/ { dataset } /datasetseries/ { series } /distributions' df = Fusion . _call_for_dataframe ( url , self . session ) if output : print ( tabulate ( df , headers = \"keys\" , tablefmt = \"psql\" )) return df","title":"list_distributions()"},{"location":"api/#fusion.fusion.Fusion.list_products","text":"Get the products contained in a catalog. A product is a grouping of datasets. Parameters: Name Type Description Default contains Union[str, list] A string or a list of strings that are product identifiers to filter the products list. If a list is provided then it will return products whose identifier matches any of the strings. Defaults to None. None id_contains bool Filter datasets only where the string(s) are contained in the identifier, ignoring description. False catalog str A catalog identifier. Defaults to 'common'. None output bool If True then print the dataframe. Defaults to False. False max_results int Limit the number of rows returned in the dataframe. Defaults to -1 which returns all results. -1 display_all_columns bool If True displays all columns returned by the API, otherwise only the key columns are displayed False Returns: Type Description DataFrame pandas.DataFrame: a dataframe with a row for each product Source code in fusion/fusion.py def list_products ( self , contains : Union [ str , list ] = None , id_contains : bool = False , catalog : str = None , output : bool = False , max_results : int = - 1 , display_all_columns : bool = False , ) -> pd . DataFrame : \"\"\"Get the products contained in a catalog. A product is a grouping of datasets. Args: contains (Union[str, list], optional): A string or a list of strings that are product identifiers to filter the products list. If a list is provided then it will return products whose identifier matches any of the strings. Defaults to None. id_contains (bool): Filter datasets only where the string(s) are contained in the identifier, ignoring description. catalog (str, optional): A catalog identifier. Defaults to 'common'. output (bool, optional): If True then print the dataframe. Defaults to False. max_results (int, optional): Limit the number of rows returned in the dataframe. Defaults to -1 which returns all results. display_all_columns (bool, optional): If True displays all columns returned by the API, otherwise only the key columns are displayed Returns: pandas.DataFrame: a dataframe with a row for each product \"\"\" catalog = self . __use_catalog ( catalog ) url = f ' { self . root_url } catalogs/ { catalog } /products' df = Fusion . _call_for_dataframe ( url , self . session ) if contains : if isinstance ( contains , list ): contains = \"|\" . join ( f ' { s } ' for s in contains ) if id_contains : df = df [ df [ 'identifier' ] . str . contains ( contains , case = False )] else : df = df [ df [ 'identifier' ] . str . contains ( contains , case = False ) | df [ 'description' ] . str . contains ( contains , case = False ) ] df [ \"category\" ] = df . category . str . join ( \", \" ) df [ \"region\" ] = df . region . str . join ( \", \" ) if not display_all_columns : df = df [[ \"identifier\" , \"title\" , \"region\" , \"category\" , \"status\" , \"description\" ]] if max_results > - 1 : df = df [ 0 : max_results ] if output : print ( tabulate ( df , headers = \"keys\" , tablefmt = \"psql\" , maxcolwidths = 30 )) return df","title":"list_products()"},{"location":"api/#fusion.fusion.Fusion.to_df","text":"Gets distributions for a specified date or date range and returns the data as a dataframe. Parameters: Name Type Description Default dataset str A dataset identifier required dt_str str Either a single date or a range identified by a start or end date, or both separated with a \":\". Defaults to 'latest' which will return the most recent instance of the dataset. 'latest' dataset_format str The file format, e.g. CSV or Parquet. Defaults to 'parquet'. 'parquet' catalog str A catalog identifier. Defaults to 'common'. None n_par int Specify how many distributions to download in parallel. Defaults to all cpus available. None show_progress bool Display a progress bar during data download Defaults to True. True columns List A list of columns to return from a parquet file. Defaults to None None filters List List[Tuple] or List[List[Tuple]] or None (default) Rows which do not match the filter predicate will be removed from scanned data. Partition keys embedded in a nested directory structure will be exploited to avoid loading files at all if they contain no matching rows. If use_legacy_dataset is True, filters can only reference partition keys and only a hive-style directory structure is supported. When setting use_legacy_dataset to False, also within-file level filtering and different partitioning schemes are supported. More on https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetDataset.html None force_download bool If True then will always download a file even if it is already on disk. Defaults to False. False download_folder str The path, absolute or relative, where downloaded files are saved. Defaults to download_folder as set in init None Returns: Type Description DataFrame pandas.DataFrame: a dataframe containing the requested data. If multiple dataset instances are retrieved then these are concatenated first. Source code in fusion/fusion.py def to_df ( self , dataset : str , dt_str : str = 'latest' , dataset_format : str = 'parquet' , catalog : str = None , n_par : int = None , show_progress : bool = True , columns : List = None , filters : List = None , force_download : bool = False , download_folder : str = None , ** kwargs , ) -> pd . DataFrame : \"\"\"Gets distributions for a specified date or date range and returns the data as a dataframe. Args: dataset (str): A dataset identifier dt_str (str, optional): Either a single date or a range identified by a start or end date, or both separated with a \":\". Defaults to 'latest' which will return the most recent instance of the dataset. dataset_format (str, optional): The file format, e.g. CSV or Parquet. Defaults to 'parquet'. catalog (str, optional): A catalog identifier. Defaults to 'common'. n_par (int, optional): Specify how many distributions to download in parallel. Defaults to all cpus available. show_progress (bool, optional): Display a progress bar during data download Defaults to True. columns (List, optional): A list of columns to return from a parquet file. Defaults to None filters (List, optional): List[Tuple] or List[List[Tuple]] or None (default) Rows which do not match the filter predicate will be removed from scanned data. Partition keys embedded in a nested directory structure will be exploited to avoid loading files at all if they contain no matching rows. If use_legacy_dataset is True, filters can only reference partition keys and only a hive-style directory structure is supported. When setting use_legacy_dataset to False, also within-file level filtering and different partitioning schemes are supported. More on https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetDataset.html force_download (bool, optional): If True then will always download a file even if it is already on disk. Defaults to False. download_folder (str, optional): The path, absolute or relative, where downloaded files are saved. Defaults to download_folder as set in __init__ Returns: pandas.DataFrame: a dataframe containing the requested data. If multiple dataset instances are retrieved then these are concatenated first. \"\"\" catalog = self . __use_catalog ( catalog ) n_par = cpu_count ( n_par ) if not download_folder : download_folder = self . download_folder download_res = self . download ( dataset , dt_str , dataset_format , catalog , n_par , show_progress , force_download , download_folder , return_paths = True ) if not all ( res [ 0 ] for res in download_res ): failed_res = [ res for res in download_res if not res [ 0 ]] raise Exception ( f \"Not all downloads were successfully completed. \" f \"Re-run to collect missing files. The following failed: \\n { failed_res } \" ) files = [ res [ 1 ] for res in download_res ] pd_read_fn_map = { 'csv' : read_csv , 'parquet' : read_parquet , 'parq' : read_parquet , 'json' : pd . read_json , } pd_read_default_kwargs : Dict [ str , Dict [ str , object ]] = { 'csv' : { 'columns' : columns , 'filters' : filters }, 'parquet' : { 'columns' : columns , 'filters' : filters }, 'json' : { 'columns' : columns , 'filters' : filters }, } pd_read_default_kwargs [ 'parq' ] = pd_read_default_kwargs [ 'parquet' ] pd_reader = pd_read_fn_map . get ( dataset_format ) pd_read_kwargs = pd_read_default_kwargs . get ( dataset_format , {}) if not pd_reader : raise Exception ( f 'No pandas function to read file in format { dataset_format } ' ) pd_read_kwargs . update ( kwargs ) if len ( files ) == 0 : raise APIResponseError ( f \"No series members for dataset: { dataset } \" f \"in date or date range: { dt_str } and format: { dataset_format } \" ) if dataset_format in [ \"parquet\" , \"parq\" ]: df = pd_reader ( files , ** pd_read_kwargs ) else : dataframes = ( pd_reader ( f , ** pd_read_kwargs ) for f in files ) df = pd . concat ( dataframes , ignore_index = True ) return df","title":"to_df()"},{"location":"api/#fusion.fusion.Fusion.upload","text":"Uploads the requested files/files to Fusion. Parameters: Name Type Description Default path str path to a file or a folder with files required dataset str Dataset name to which the file will be uplaoded (for single file only). If not provided the dataset will be implied from file's name. None dt_str str A single date. Defaults to 'latest' which will return the most recent. Relevant for a single file upload only. If not provided the dataset will be implied from file's name. 'latest' catalog str A catalog identifier. Defaults to 'common'. None n_par int Specify how many distributions to download in parallel. Defaults to all cpus available. None show_progress bool Display a progress bar during data download Defaults to True. True return_paths bool Return paths and success statuses of the downloaded files. False Source code in fusion/fusion.py def upload ( self , path : str , dataset : str = None , dt_str : str = 'latest' , catalog : str = None , n_par : int = None , show_progress : bool = True , return_paths : bool = False , ): \"\"\"Uploads the requested files/files to Fusion. Args: path (str): path to a file or a folder with files dataset (str, optional): Dataset name to which the file will be uplaoded (for single file only). If not provided the dataset will be implied from file's name. dt_str (str, optional): A single date. Defaults to 'latest' which will return the most recent. Relevant for a single file upload only. If not provided the dataset will be implied from file's name. catalog (str, optional): A catalog identifier. Defaults to 'common'. n_par (int, optional): Specify how many distributions to download in parallel. Defaults to all cpus available. show_progress (bool, optional): Display a progress bar during data download Defaults to True. return_paths (bool, optional): Return paths and success statuses of the downloaded files. Returns: \"\"\" catalog = self . __use_catalog ( catalog ) if not self . fs . exists ( path ): raise RuntimeError ( \"The provided path does not exist\" ) fs_fusion = self . get_fusion_filesystem () if self . fs . info ( path )[ \"type\" ] == \"directory\" : file_path_lst = self . fs . find ( path ) local_file_validation = validate_file_names ( file_path_lst , fs_fusion ) file_path_lst = [ f for flag , f in zip ( local_file_validation , file_path_lst ) if flag ] local_url_eqiv = [ path_to_url ( i ) for i in file_path_lst ] else : file_path_lst = [ path ] if not catalog or not dataset : local_file_validation = validate_file_names ( file_path_lst , fs_fusion ) file_path_lst = [ f for flag , f in zip ( local_file_validation , file_path_lst ) if flag ] local_url_eqiv = [ path_to_url ( i ) for i in file_path_lst ] else : file_format = path . split ( \".\" )[ - 1 ] dt_str = dt_str if dt_str != \"latest\" else pd . Timestamp ( \"today\" ) . date () . strftime ( \"%Y%m %d \" ) dt_str = pd . Timestamp ( dt_str ) . date () . strftime ( \"%Y%m %d \" ) if catalog not in fs_fusion . ls ( '' ) or dataset not in [ i . split ( '/' )[ - 1 ] for i in fs_fusion . ls ( f \" { catalog } /datasets\" )]: msg = f \"File file has not been uploaded, one of the catalog: { catalog } \" \\ f \"or dataset: { dataset } does not exit.\" warnings . warn ( msg ) return [( False , path , Exception ( msg ))] local_url_eqiv = [ path_to_url ( f \" { dataset } __ { catalog } __ { dt_str } . { file_format } \" )] df = pd . DataFrame ([ file_path_lst , local_url_eqiv ]) . T df . columns = [ \"path\" , \"url\" ] if show_progress : loop = tqdm ( df . iterrows (), total = len ( df )) else : loop = df . iterrows () n_par = cpu_count ( n_par ) parallel = True if len ( df ) > 1 else False res = upload_files ( fs_fusion , self . fs , loop , parallel = parallel , n_par = n_par ) if not all ( r [ 0 ] for r in res ): failed_res = [ r for r in res if not r [ 0 ]] msg = f \"Not all uploads were successfully completed. The following failed: \\n { failed_res } \" logger . warning ( msg ) warnings . warn ( msg ) return res if return_paths else None","title":"upload()"},{"location":"api/#fusion.fusion_filesystem","text":"Fusion FileSystem.","title":"fusion_filesystem"},{"location":"api/#fusion.fusion_filesystem.FusionHTTPFileSystem","text":"Fusion HTTP filesystem.","title":"FusionHTTPFileSystem"},{"location":"api/#fusion.fusion_filesystem.FusionHTTPFileSystem.__init__","text":"Same signature as the fsspec HTTPFileSystem. Parameters: Name Type Description Default credentials Credentials. 'config/client_credentials.json' *args Args. () **kwargs Kwargs. {} Source code in fusion/fusion_filesystem.py def __init__ ( self , credentials = 'config/client_credentials.json' , * args , ** kwargs ): \"\"\"Same signature as the fsspec HTTPFileSystem. Args: credentials: Credentials. *args: Args. **kwargs: Kwargs. \"\"\" self . credentials = credentials if \"get_client\" not in kwargs : kwargs [ \"get_client\" ] = get_client if \"client_kwargs\" not in kwargs : if isinstance ( credentials , FusionCredentials ): self . credentials = credentials else : self . credentials = FusionCredentials . from_object ( credentials ) kwargs [ \"client_kwargs\" ] = { \"credentials\" : self . credentials , \"root_url\" : \"https://fusion-api.jpmorgan.com/fusion/v1/\" } if self . credentials . proxies : if \"http\" in self . credentials . proxies . keys (): kwargs [ \"proxy\" ] = self . credentials . proxies [ \"http\" ] elif \"https\" in self . credentials . proxies . keys (): kwargs [ \"proxy\" ] = self . credentials . proxies [ \"https\" ] if \"headers\" not in kwargs : kwargs [ \"headers\" ] = { \"Accept-Encoding\" : \"identity\" } super () . __init__ ( * args , ** kwargs )","title":"__init__()"},{"location":"api/#fusion.fusion_filesystem.FusionHTTPFileSystem.cat","text":"Fetch paths' contents. Parameters: Name Type Description Default url Url. required start Start. None end End. None **kwargs Kwargs. {} Source code in fusion/fusion_filesystem.py def cat ( self , url , start = None , end = None , ** kwargs ): \"\"\"Fetch paths' contents. Args: url: Url. start: Start. end: End. **kwargs: Kwargs. Returns: \"\"\" url = self . _decorate_url ( url ) return super () . cat ( url , start = start , end = end , ** kwargs )","title":"cat()"},{"location":"api/#fusion.fusion_filesystem.FusionHTTPFileSystem.exists","text":"Check existence. Parameters: Name Type Description Default url Url. required detail Detail. True **kwargs Kwargs. {} Source code in fusion/fusion_filesystem.py def exists ( self , url , detail = True , ** kwargs ): \"\"\"Check existence. Args: url: Url. detail: Detail. **kwargs: Kwargs. Returns: \"\"\" url = self . _decorate_url ( url ) return super () . exists ( url , ** kwargs )","title":"exists()"},{"location":"api/#fusion.fusion_filesystem.FusionHTTPFileSystem.find","text":"Find all file in a folder. Parameters: Name Type Description Default path Path. required maxdepth Max depth. None withdirs With dirs, default to False. False **kwargs Kwargs. {} Source code in fusion/fusion_filesystem.py def find ( self , path , maxdepth = None , withdirs = False , ** kwargs ): \"\"\"Find all file in a folder. Args: path: Path. maxdepth: Max depth. withdirs: With dirs, default to False. **kwargs: Kwargs. Returns: \"\"\" path = self . _decorate_url ( path ) return super () . find ( path , maxdepth = maxdepth , withdirs = withdirs , ** kwargs )","title":"find()"},{"location":"api/#fusion.fusion_filesystem.FusionHTTPFileSystem.get","text":"Copy file(s) to local. Parameters: Name Type Description Default rpath Rpath. required lpath Lpath. required chunk_size Chunk size. 5242880 callback Callback function. <fsspec.callbacks.NoOpCallback object at 0x7f6ab1c21e50> **kwargs Kwargs. {} Source code in fusion/fusion_filesystem.py def get ( self , rpath , lpath , chunk_size = 5 * 2 ** 20 , callback = _DEFAULT_CALLBACK , ** kwargs ): \"\"\"Copy file(s) to local. Args: rpath: Rpath. lpath: Lpath. chunk_size: Chunk size. callback: Callback function. **kwargs: Kwargs. Returns: \"\"\" rpath = self . _decorate_url ( rpath ) return super () . get ( rpath , lpath , chunk_size = 5 * 2 ** 20 , callback = _DEFAULT_CALLBACK , ** kwargs )","title":"get()"},{"location":"api/#fusion.fusion_filesystem.FusionHTTPFileSystem.glob","text":"Glob. Parameters: Name Type Description Default path Path. required **kwargs Kwargs. {} Source code in fusion/fusion_filesystem.py def glob ( self , path , ** kwargs ): \"\"\"Glob. Args: path: Path. **kwargs: Kwargs. Returns: \"\"\" return super () . glob ( path , ** kwargs )","title":"glob()"},{"location":"api/#fusion.fusion_filesystem.FusionHTTPFileSystem.info","text":"Return info. Parameters: Name Type Description Default path Path. required **kwargs Kwargs. {} Source code in fusion/fusion_filesystem.py def info ( self , path , ** kwargs ): \"\"\"Return info. Args: path: Path. **kwargs: Kwargs. Returns: \"\"\" path = self . _decorate_url ( path ) kwargs [ \"keep_protocol\" ] = True res = super () . ls ( path , detail = True , ** kwargs )[ 0 ] if res [ \"type\" ] == \"file\" : return res else : return super () . info ( path , ** kwargs )","title":"info()"},{"location":"api/#fusion.fusion_filesystem.FusionHTTPFileSystem.isfile","text":"Is path a file. Parameters: Name Type Description Default path Path. required Source code in fusion/fusion_filesystem.py def isfile ( self , path ): \"\"\"Is path a file. Args: path: Path. Returns: \"\"\" path = self . _decorate_url ( path ) return super () . isfile ( path )","title":"isfile()"},{"location":"api/#fusion.fusion_filesystem.FusionHTTPFileSystem.ls","text":"List resources. Parameters: Name Type Description Default url Url. required detail Detail. False **kwargs Kwargs. {} Source code in fusion/fusion_filesystem.py def ls ( self , url , detail = False , ** kwargs ): \"\"\"List resources. Args: url: Url. detail: Detail. **kwargs: Kwargs. Returns: \"\"\" url = self . _decorate_url ( url ) ret = super () . ls ( url , detail = detail , ** kwargs ) keep_protocol = kwargs . pop ( \"keep_protocol\" , False ) if detail : if not keep_protocol : for k in ret : k [ \"name\" ] = k [ \"name\" ] . split ( f ' { self . client_kwargs [ \"root_url\" ] } catalogs/' )[ - 1 ] else : if not keep_protocol : return [ x . split ( f ' { self . client_kwargs [ \"root_url\" ] } catalogs/' )[ - 1 ] for x in ret ] return ret","title":"ls()"},{"location":"api/#fusion.fusion_filesystem.FusionHTTPFileSystem.open","text":"Open. Parameters: Name Type Description Default path Path. required mode Defaults to rb. 'rb' **kwargs Kwargs. {} Source code in fusion/fusion_filesystem.py def open ( self , path , mode = \"rb\" , ** kwargs , ): \"\"\"Open. Args: path: Path. mode: Defaults to rb. **kwargs: Kwargs. Returns: \"\"\" path = self . _decorate_url ( path ) return super () . open ( path , mode , ** kwargs )","title":"open()"},{"location":"api/#fusion.fusion_filesystem.FusionHTTPFileSystem.put","text":"Copy file(s) from local. Parameters: Name Type Description Default lpath Lpath. required rpath Rpath. required chunk_size Chunk size. 5242880 callback Callback function. <fsspec.callbacks.NoOpCallback object at 0x7f6ab1c21e50> method 'put' **kwargs Kwargs. {} Source code in fusion/fusion_filesystem.py def put ( self , lpath , rpath , chunk_size = 5 * 2 ** 20 , callback = _DEFAULT_CALLBACK , method = \"put\" , ** kwargs ): \"\"\"Copy file(s) from local. Args: lpath: Lpath. rpath: Rpath. chunk_size: Chunk size. callback: Callback function. method: Method: put/post. **kwargs: Kwargs. Returns: \"\"\" rpath = self . _decorate_url ( rpath ) args = [ lpath , rpath , chunk_size , callback , method ] return sync ( super () . loop , super () . _put_file , * args , ** kwargs )","title":"put()"},{"location":"api/#fusion.utils","text":"Fusion utilities.","title":"utils"},{"location":"api/#fusion.utils.cpu_count","text":"Determine the number of cpus/threads for parallelization. Parameters: Name Type Description Default thread_pool_size int override argument for number of cpus/threads. None Source code in fusion/utils.py def cpu_count ( thread_pool_size : int = None ): \"\"\"Determine the number of cpus/threads for parallelization. Args: thread_pool_size: override argument for number of cpus/threads. Returns: number of cpus/threads to use. \"\"\" if os . environ . get ( \"NUM_THREADS\" ) is not None : return int ( os . environ [ \"NUM_THREADS\" ]) if thread_pool_size : return thread_pool_size else : if mp . cpu_count (): thread_pool_size = mp . cpu_count () else : thread_pool_size = DEFAULT_THREAD_POOL_SIZE return thread_pool_size","title":"cpu_count()"},{"location":"api/#fusion.utils.distribution_to_filename","text":"Returns a filename representing a dataset distribution. Parameters: Name Type Description Default root_folder str The root folder path. required dataset str The dataset identifier. required datasetseries str The datasetseries instance identifier. required file_format str The file type, e.g. CSV or Parquet required catalog str The data catalog containing the dataset. Defaults to \"common\". 'common' Returns: Type Description Path Source code in fusion/utils.py def distribution_to_filename ( root_folder : str , dataset : str , datasetseries : str , file_format : str , catalog : str = 'common' ) -> Path : \"\"\"Returns a filename representing a dataset distribution. Args: root_folder (str): The root folder path. dataset (str): The dataset identifier. datasetseries (str): The datasetseries instance identifier. file_format (str): The file type, e.g. CSV or Parquet catalog (str, optional): The data catalog containing the dataset. Defaults to \"common\". Returns: tuple: A tuple of dates. \"\"\" if datasetseries [ - 1 ] == '/' or datasetseries [ - 1 ] == ' \\\\ ' : datasetseries = datasetseries [ 0 : - 1 ] file_name = f \" { dataset } __ { catalog } __ { datasetseries } . { file_format } \" return Path ( root_folder , file_name )","title":"distribution_to_filename()"},{"location":"api/#fusion.utils.distribution_to_url","text":"Returns the API URL to download a dataset distribution. Parameters: Name Type Description Default root_url str The base url for the API. required dataset str The dataset identifier. required datasetseries str The datasetseries instance identifier. required file_format str The file type, e.g. CSV or Parquet required catalog str The data catalog containing the dataset. Defaults to \"common\". 'common' Returns: Type Description str Source code in fusion/utils.py def distribution_to_url ( root_url : str , dataset : str , datasetseries : str , file_format : str , catalog : str = 'common' ) -> str : \"\"\"Returns the API URL to download a dataset distribution. Args: root_url (str): The base url for the API. dataset (str): The dataset identifier. datasetseries (str): The datasetseries instance identifier. file_format (str): The file type, e.g. CSV or Parquet catalog (str, optional): The data catalog containing the dataset. Defaults to \"common\". Returns: str: A URL for the API distribution endpoint. \"\"\" if datasetseries [ - 1 ] == '/' or datasetseries [ - 1 ] == ' \\\\ ' : datasetseries = datasetseries [ 0 : - 1 ] return f \" { root_url } catalogs/ { catalog } /datasets/ { dataset } /datasetseries/ { datasetseries } /distributions/ { file_format } \"","title":"distribution_to_url()"},{"location":"api/#fusion.utils.get_client","text":"Gets session for async. Parameters: Name Type Description Default credentials Credentials. required **kwargs Kwargs. {} Source code in fusion/utils.py async def get_client ( credentials , ** kwargs ): \"\"\"Gets session for async. Args: credentials: Credentials. **kwargs: Kwargs. Returns: \"\"\" async def on_request_start ( session , trace_config_ctx , params ): payload = ( { \"grant_type\" : \"client_credentials\" , \"client_id\" : credentials . client_id , \"client_secret\" : credentials . client_secret , \"aud\" : credentials . resource , } if credentials . grant_type == 'client_credentials' else { \"grant_type\" : \"password\" , \"client_id\" : credentials . client_id , \"username\" : credentials . username , \"password\" : credentials . password , \"resource\" : credentials . resource , } ) async with aiohttp . ClientSession () as session : if credentials . proxies : response = await session . post ( credentials . auth_url , data = payload , proxy = http_proxy ) else : response = await session . post ( credentials . auth_url , data = payload ) response_data = await response . json () access_token = response_data [ \"access_token\" ] params . headers . update ({ 'Authorization' : f 'Bearer { access_token } ' }) if credentials . proxies : if \"http\" in credentials . proxies . keys (): http_proxy = credentials . proxies [ \"http\" ] elif \"https\" in credentials . proxies . keys (): http_proxy = credentials . proxies [ \"https\" ] trace_config = aiohttp . TraceConfig () trace_config . on_request_start . append ( on_request_start ) return aiohttp . ClientSession ( trace_configs = [ trace_config ], trust_env = True )","title":"get_client()"},{"location":"api/#fusion.utils.get_session","text":"Create a new http session and set parameters. Parameters: Name Type Description Default credentials FusionCredentials Valid user credentials to provide an acces token required root_url str The URL to call. required Returns: Type Description Session requests.Session(): A HTTP Session object Source code in fusion/utils.py def get_session ( credentials : FusionCredentials , root_url : str , get_retries : Union [ int , Retry ] = None ) -> requests . Session : \"\"\"Create a new http session and set parameters. Args: credentials (FusionCredentials): Valid user credentials to provide an acces token root_url (str): The URL to call. Returns: requests.Session(): A HTTP Session object \"\"\" if not get_retries : get_retries = Retry ( total = 5 , backoff_factor = 0.1 , status_forcelist = [ 429 , 500 , 502 , 503 , 504 ]) else : get_retries = Retry . from_int ( get_retries ) session = requests . Session () auth_handler = FusionOAuthAdapter ( credentials , max_retries = get_retries ) if credentials . proxies : # mypy does note recognise session.proxies as a dict so fails this line, we'll ignore this chk session . proxies . update ( credentials . proxies ) # type:ignore try : mount_url = _get_canonical_root_url ( root_url ) except Exception : mount_url = \"https://\" session . mount ( mount_url , auth_handler ) return session","title":"get_session()"},{"location":"api/#fusion.utils.normalise_dt_param_str","text":"Convert a date parameter which may be a single date or a date range into a tuple. Parameters: Name Type Description Default dt str Either a single date or a date range separated by a \":\". required Returns: Type Description tuple Source code in fusion/utils.py def normalise_dt_param_str ( dt : str ) -> tuple : \"\"\"Convert a date parameter which may be a single date or a date range into a tuple. Args: dt (str): Either a single date or a date range separated by a \":\". Returns: tuple: A tuple of dates. \"\"\" date_parts = dt . split ( \":\" ) if not date_parts or len ( date_parts ) > 2 : raise ValueError ( f \"Unable to parse { dt } as either a date or an interval\" ) return tuple (( _normalise_dt_param ( dt_part ) if dt_part else dt_part for dt_part in date_parts ))","title":"normalise_dt_param_str()"},{"location":"api/#fusion.utils.path_to_url","text":"Convert file name to fusion url. Parameters: Name Type Description Default x str File path. required Returns (str): Fusion url string. Source code in fusion/utils.py def path_to_url ( x ): \"\"\"Convert file name to fusion url. Args: x (str): File path. Returns (str): Fusion url string. \"\"\" catalog , dataset , date , ext = _filename_to_distribution ( x . split ( \"/\" )[ - 1 ]) return \"/\" . join ( distribution_to_url ( \"\" , dataset , date , ext , catalog ) . split ( \"/\" )[ 1 :])","title":"path_to_url()"},{"location":"api/#fusion.utils.read_csv","text":"Reads csv with possibility of selecting columns and filtering the data. Parameters: Name Type Description Default path str path to the csv file. required columns list list of selected fields. None filters list filters. None fs filesystem object. None Returns: Type Description pandas.DataFrame a dataframe containing the data. Source code in fusion/utils.py def read_csv ( path : str , columns : list = None , filters : list = None , fs = None ): \"\"\"Reads csv with possibility of selecting columns and filtering the data. Args: path (str): path to the csv file. columns: list of selected fields. filters: filters. fs: filesystem object. Returns: pandas.DataFrame: a dataframe containing the data. \"\"\" try : try : tbl = _csv_to_table ( path , fs ) except Exception as err : logger . log ( VERBOSE_LVL , f 'Failed to read { path } , with comma delimiter. { err } ' , ) raise Exception out = BytesIO () pq . write_table ( tbl , out ) del tbl res = pq . read_table ( out , filters = filters , columns = columns ) . to_pandas () except Exception as err : logger . log ( VERBOSE_LVL , f \"Could not parse { path } properly. \" f \"Trying with pandas csv reader. { err } \" , ) try : with ( fs . open ( path ) if fs else nullcontext ( path )) as f : res = pd . read_csv ( f , usecols = columns , index_col = False ) except Exception as err : logger . log ( VERBOSE_LVL , f \"Could not parse { path } properly. \" f \"Trying with pandas csv reader pandas engine. { err } \" , ) with ( fs . open ( path ) if fs else nullcontext ( path )) as f : res = pd . read_table ( f , usecols = columns , index_col = False , engine = \"python\" , delimiter = None ) return res","title":"read_csv()"},{"location":"api/#fusion.utils.read_parquet","text":"Read parquet files(s) to pandas. Parameters: Name Type Description Default path Union[list, str] path or a list of paths to parquet files. required columns list list of selected fields. None filters list filters. None fs filesystem object. None Returns: Type Description pandas.DataFrame a dataframe containing the data. Source code in fusion/utils.py def read_parquet ( path : Union [ list , str ], columns : list = None , filters : list = None , fs = None ): \"\"\"Read parquet files(s) to pandas. Args: path (Union[list, str]): path or a list of paths to parquet files. columns (list): list of selected fields. filters (list): filters. fs: filesystem object. Returns: pandas.DataFrame: a dataframe containing the data. \"\"\" return pq . ParquetDataset ( path , use_legacy_dataset = False , filters = filters , filesystem = fs , memory_map = True ) . read_pandas ( columns = columns ) . to_pandas ()","title":"read_parquet()"},{"location":"api/#fusion.utils.stream_single_file_new_session","text":"Function to stream a single file from the API to a file on disk. Parameters: Name Type Description Default credentials FusionCredentials Valid user credentials to provide an acces token required url str The URL to call. required output_file str The filename that the data will be saved into. required overwrite bool True if previously downloaded files should be overwritten. Defaults to True. True block_size int The chunk size to download data. Defaults to DEFAULT_CHUNK_SIZE 65536 dry_run bool Test that a file can be downloaded and return the filename without downloading the data. Defaults to False. False fs fsspec.filesystem Filesystem. None Returns: Type Description tuple Source code in fusion/utils.py def stream_single_file_new_session ( credentials , url : str , output_file : str , overwrite : bool = True , block_size = DEFAULT_CHUNK_SIZE , dry_run : bool = False , fs = None ) -> tuple : \"\"\"Function to stream a single file from the API to a file on disk. Args: credentials (FusionCredentials): Valid user credentials to provide an acces token url (str): The URL to call. output_file (str): The filename that the data will be saved into. overwrite (bool, optional): True if previously downloaded files should be overwritten. Defaults to True. block_size (int, optional): The chunk size to download data. Defaults to DEFAULT_CHUNK_SIZE dry_run (bool, optional): Test that a file can be downloaded and return the filename without downloading the data. Defaults to False. fs (fsspec.filesystem): Filesystem. Returns: tuple: A tuple \"\"\" if dry_run : return _stream_single_file_new_session_dry_run ( credentials , url , output_file ) output_file_path = Path ( output_file ) if not overwrite and output_file_path . exists (): return ( True , output_file , None ) tmp_name = output_file_path . with_name ( output_file_path . name + \".tmp\" ) try : with get_session ( credentials , url ) . get ( url , stream = True ) as r : r . raise_for_status () byte_cnt = 0 with fs . open ( tmp_name , \"wb\" ) as outfile : for chunk in r . iter_content ( block_size ): byte_cnt += len ( chunk ) outfile . write ( chunk ) tmp_name . rename ( output_file_path ) try : tmp_name . unlink () except FileNotFoundError : pass logger . log ( VERBOSE_LVL , f 'Wrote { byte_cnt : , } bytes to { output_file_path } , via { tmp_name } ' , ) return ( True , output_file , None ) except Exception as ex : logger . log ( VERBOSE_LVL , f 'Failed to write to { output_file_path } , via { tmp_name } . ex - { ex } ' , ) return False , output_file , ex","title":"stream_single_file_new_session()"},{"location":"api/#fusion.utils.upload_files","text":"Upload file into Fusion. Parameters: Name Type Description Default fs_fusion Fusion filesystem. required fs_local Local filesystem. required loop iterable Loop of files to iterate through. required parallel bool Is parallel mode enabled. True n_par int Number of subprocesses. -1 Source code in fusion/utils.py def upload_files ( fs_fusion , fs_local , loop , parallel = True , n_par =- 1 ): \"\"\"Upload file into Fusion. Args: fs_fusion: Fusion filesystem. fs_local: Local filesystem. loop (iterable): Loop of files to iterate through. parallel (bool): Is parallel mode enabled. n_par (int): Number of subprocesses. Returns: List of update statuses. \"\"\" def _upload ( row ): kw = { \"headers\" : _construct_headers ( fs_local , row )} p_url = row [ \"url\" ] try : with fs_local . open ( row [ \"path\" ], \"rb\" ) as file_local : fs_fusion . put ( file_local , p_url , chunk_size = 100 * 2 ** 20 , method = \"put\" , ** kw ) return True , row [ \"path\" ], None except Exception as ex : logger . log ( VERBOSE_LVL , f 'Failed to upload { row [ \"path\" ] } . ex - { ex } ' , ) return False , row [ \"path\" ], ex if parallel : res = Parallel ( n_jobs = n_par )( delayed ( _upload )( row ) for index , row in loop ) else : res = [ _upload ( row ) for index , row in loop ] return res","title":"upload_files()"},{"location":"api/#fusion.utils.validate_file_names","text":"Validate if the file name format adheres to the standard. Parameters: Name Type Description Default paths list List of file paths. required fs_fusion Fusion filesystem. required Returns (list): List of booleans. Source code in fusion/utils.py def validate_file_names ( paths , fs_fusion ): \"\"\"Validate if the file name format adheres to the standard. Args: paths (list): List of file paths. fs_fusion: Fusion filesystem. Returns (list): List of booleans. \"\"\" file_names = [ i . split ( \"/\" )[ - 1 ] . split ( \".\" )[ 0 ] for i in paths ] validation = [] all_catalogs = fs_fusion . ls ( '' ) all_datasets = {} for f_n in file_names : tmp = f_n . split ( '__' ) if len ( tmp ) == 3 : val = tmp [ 1 ] in all_catalogs if not val : validation . append ( False ) else : if tmp [ 1 ] not in all_datasets . keys (): all_datasets [ tmp [ 1 ]] = [ i . split ( '/' )[ - 1 ] for i in fs_fusion . ls ( f \" { tmp [ 1 ] } /datasets\" )] val = tmp [ 0 ] in all_datasets [ tmp [ 1 ]] validation . append ( val ) else : validation . append ( False ) if not all ( validation ): for i , p in enumerate ( paths ): if not validation [ i ]: logger . warning ( f \"The file in { p } has a non-compliant name and will not be processed. \" f \"Please rename the file to dataset__catalog__yyyymmdd.format\" ) return validation","title":"validate_file_names()"},{"location":"changelog/","text":"Changelog \u00b6 All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . [1.0.6] - 2022-11-21 \u00b6 Support setting of the default catalog Fusion filesystem module Upload functionality Folder traversing for credentials Filters for parquet and csv file opening [1.0.5] - 2022-06-22 \u00b6 Add support for internal auth methods [1.0.4] - 2022-05-19 \u00b6 Support proxy servers in auth post requests Add back support for '2020-01-01' and '20200101' date formats Various bug fixes Streamline credentials creation [1.0.3] - 2022-05-12 \u00b6 Add support for 'latest' datasets [1.0.2] - 2022-05-12 \u00b6 Integrate build with docs [1.0.1] - 2022-05-12 \u00b6 First live release on JPMC gitub","title":"Changelog"},{"location":"changelog/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"changelog/#106---2022-11-21","text":"Support setting of the default catalog Fusion filesystem module Upload functionality Folder traversing for credentials Filters for parquet and csv file opening","title":"[1.0.6] - 2022-11-21"},{"location":"changelog/#105---2022-06-22","text":"Add support for internal auth methods","title":"[1.0.5] - 2022-06-22"},{"location":"changelog/#104---2022-05-19","text":"Support proxy servers in auth post requests Add back support for '2020-01-01' and '20200101' date formats Various bug fixes Streamline credentials creation","title":"[1.0.4] - 2022-05-19"},{"location":"changelog/#103---2022-05-12","text":"Add support for 'latest' datasets","title":"[1.0.3] - 2022-05-12"},{"location":"changelog/#102---2022-05-12","text":"Integrate build with docs","title":"[1.0.2] - 2022-05-12"},{"location":"changelog/#101---2022-05-12","text":"First live release on JPMC gitub","title":"[1.0.1] - 2022-05-12"},{"location":"contributing/","text":"Contributing \u00b6 Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways: Types of Contributions \u00b6 Report Bugs \u00b6 Report bugs at https://github.com/{{ cookiecutter.github_username }}/{{ cookiecutter.project_slug }}/issues. If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug. Fix Bugs \u00b6 Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it. Implement Features \u00b6 Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it. Write Documentation \u00b6 {{ cookiecutter.project_name }} could always use more documentation, whether as part of the official {{ cookiecutter.project_name }} docs, in docstrings, or even on the web in blog posts, articles, and such. Submit Feedback \u00b6 The best way to send feedback is to file an issue at https://github.com/{{ cookiecutter.github_username }}/{{ cookiecutter.project_slug }}/issues. If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :) Get Started! \u00b6 Ready to contribute? Here's how to set up {{ cookiecutter.project_slug }} for local development. Fork the {{ cookiecutter.project_slug }} repo on GitHub. Clone your fork locally $ git clone git@github.com:your_name_here/{{ cookiecutter.project_slug }}.git Ensure poetry is installed. Install dependencies and start your virtualenv: $ poetry install -E test -E doc -E dev Create a branch for local development: $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass the tests, including testing other Python versions, with tox: $ poetry run tox Commit your changes and push your branch to GitHub: $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website. Pull Request Guidelines \u00b6 Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md. The pull request should work for Python 3.7, 3.8 and 3.9. Check https://github.com/{{ cookiecutter.github_username }}/{{ cookiecutter.project_slug }}/actions and make sure that the tests pass for all supported Python versions. Tips \u00b6 $ poetry run pytest tests/test_{{ cookiecutter.pkg_name }}.py To run a subset of tests. Deploying \u00b6 A reminder for the maintainers on how to deploy. Make sure all your changes are committed (including an entry in CHANGELOG.md). Then run: $ poetry run bump2version patch # possible: major / minor / patch $ git push $ git push --tags GitHub Actions will then deploy to PyPI if tests pass.","title":"Contributing"},{"location":"contributing/#contributing","text":"Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways:","title":"Contributing"},{"location":"contributing/#types-of-contributions","text":"","title":"Types of Contributions"},{"location":"contributing/#report-bugs","text":"Report bugs at https://github.com/{{ cookiecutter.github_username }}/{{ cookiecutter.project_slug }}/issues. If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug.","title":"Report Bugs"},{"location":"contributing/#fix-bugs","text":"Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.","title":"Fix Bugs"},{"location":"contributing/#implement-features","text":"Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.","title":"Implement Features"},{"location":"contributing/#write-documentation","text":"{{ cookiecutter.project_name }} could always use more documentation, whether as part of the official {{ cookiecutter.project_name }} docs, in docstrings, or even on the web in blog posts, articles, and such.","title":"Write Documentation"},{"location":"contributing/#submit-feedback","text":"The best way to send feedback is to file an issue at https://github.com/{{ cookiecutter.github_username }}/{{ cookiecutter.project_slug }}/issues. If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :)","title":"Submit Feedback"},{"location":"contributing/#get-started","text":"Ready to contribute? Here's how to set up {{ cookiecutter.project_slug }} for local development. Fork the {{ cookiecutter.project_slug }} repo on GitHub. Clone your fork locally $ git clone git@github.com:your_name_here/{{ cookiecutter.project_slug }}.git Ensure poetry is installed. Install dependencies and start your virtualenv: $ poetry install -E test -E doc -E dev Create a branch for local development: $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass the tests, including testing other Python versions, with tox: $ poetry run tox Commit your changes and push your branch to GitHub: $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website.","title":"Get Started!"},{"location":"contributing/#pull-request-guidelines","text":"Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md. The pull request should work for Python 3.7, 3.8 and 3.9. Check https://github.com/{{ cookiecutter.github_username }}/{{ cookiecutter.project_slug }}/actions and make sure that the tests pass for all supported Python versions.","title":"Pull Request Guidelines"},{"location":"contributing/#tips","text":"$ poetry run pytest tests/test_{{ cookiecutter.pkg_name }}.py To run a subset of tests.","title":"Tips"},{"location":"contributing/#deploying","text":"A reminder for the maintainers on how to deploy. Make sure all your changes are committed (including an entry in CHANGELOG.md). Then run: $ poetry run bump2version patch # possible: major / minor / patch $ git push $ git push --tags GitHub Actions will then deploy to PyPI if tests pass.","title":"Deploying"},{"location":"installation/","text":"Installation \u00b6 Stable release \u00b6 To install PyFusion, run this command in your terminal: $ pip install pyfusion This is the preferred method to install PyFusion, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process. From source \u00b6 The source for PyFusion can be downloaded from the Github repo . You can either clone the public repository: $ git clone git://github.com/jpmorganchase/fusion Or download the tarball : $ curl -OJL https://github.com/jpmorganchase/fusion/tarball/master Once you have a copy of the source, you can install it with: $ pip install .","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#stable-release","text":"To install PyFusion, run this command in your terminal: $ pip install pyfusion This is the preferred method to install PyFusion, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process.","title":"Stable release"},{"location":"installation/#from-source","text":"The source for PyFusion can be downloaded from the Github repo . You can either clone the public repository: $ git clone git://github.com/jpmorganchase/fusion Or download the tarball : $ curl -OJL https://github.com/jpmorganchase/fusion/tarball/master Once you have a copy of the source, you can install it with: $ pip install .","title":"From source"},{"location":"usage/","text":"PyFusion \u00b6 PyFusion is the Python SDK for the Fusion platform API. Installation \u00b6 pip install pyfusion","title":"Usage"},{"location":"usage/#pyfusion","text":"PyFusion is the Python SDK for the Fusion platform API.","title":"PyFusion"},{"location":"usage/#installation","text":"pip install pyfusion","title":"Installation"}]}